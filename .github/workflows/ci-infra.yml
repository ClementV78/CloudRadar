name: ci-infra

# Workflow overview:
# - PR: fmt/validate/plan/module validation/security scan.
# - Dispatch: select env -> validate -> plan -> apply -> load outputs -> k3s check -> ArgoCD bootstrap -> smoke tests.

on:
  pull_request:
    paths:
      - "infra/**"
      - ".github/workflows/ci-infra.yml"
  workflow_dispatch:
    inputs:
      environment:
        description: "Target environment to apply (dev or prod)"
        required: true
        type: choice
        options:
          - dev
          - prod
      redis_backup_restore:
        description: "Restore Redis from latest backup after deploy (dev only)"
        required: true
        type: boolean
        default: true
      auto_approve:
        description: "Set to true to run apply"
        required: true
        type: boolean
        default: false
      run_smoke_tests:
        description: "Set to true to run post-apply network smoke tests"
        required: true
        type: boolean
        default: false
      backup_bucket_name:
        description: "Optional S3 bucket name for SQLite backups (dev only)"
        required: false

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: ${{ vars.AWS_REGION || 'us-east-1' }}
  TF_STATE_BUCKET: ${{ vars.TF_STATE_BUCKET }}
  TF_LOCK_TABLE_NAME: ${{ vars.TF_LOCK_TABLE_NAME }}
  TF_BACKUP_BUCKET_NAME: ${{ vars.TF_BACKUP_BUCKET_NAME }}
  TF_ROLE_ARN: ${{ vars.AWS_TERRAFORM_ROLE_ARN }}

jobs:
  fmt:
    if: ${{ github.event_name == 'pull_request' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Terraform fmt
        run: terraform fmt -check -recursive infra/aws

  validate-plan:
    if: ${{ github.event_name == 'pull_request' }}
    runs-on: ubuntu-latest
    needs: fmt
    strategy:
      matrix:
        include:
          - name: bootstrap
            dir: infra/aws/bootstrap
            backend: false
          - name: live-dev
            dir: infra/aws/live/dev
            backend: true
            key: cloudradar/dev/terraform.tfstate
          - name: live-prod
            dir: infra/aws/live/prod
            backend: true
            key: cloudradar/prod/terraform.tfstate
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Terraform init (local backend)
        if: ${{ matrix.backend == false }}
        run: terraform -chdir=${{ matrix.dir }} init -backend=false

      - name: Terraform init (remote backend)
        if: ${{ matrix.backend == true }}
        run: |
          terraform -chdir=${{ matrix.dir }} init \
            -backend-config="bucket=${TF_STATE_BUCKET}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=${TF_LOCK_TABLE_NAME}" \
            -backend-config="key=${{ matrix.key }}"

      - name: Terraform validate
        run: terraform -chdir=${{ matrix.dir }} validate

      - name: Terraform plan
        if: ${{ matrix.name == 'bootstrap' }}
        run: |
          set -euo pipefail
          BACKUP_BUCKET_NAME="${TF_BACKUP_BUCKET_NAME:-}"
          EXTRA_VARS=()

          if [[ -n "${BACKUP_BUCKET_NAME}" ]]; then
            EXTRA_VARS+=("-var=backup_bucket_name=${BACKUP_BUCKET_NAME}")
          fi

          terraform -chdir=${{ matrix.dir }} plan -input=false -no-color \
            -var="region=${AWS_REGION}" \
            -var="state_bucket_name=${TF_STATE_BUCKET}" \
            -var="lock_table_name=${TF_LOCK_TABLE_NAME}" \
            "${EXTRA_VARS[@]}"

      - name: Terraform plan
        if: ${{ matrix.name != 'bootstrap' }}
        run: |
          set -euo pipefail
          BACKUP_BUCKET_NAME="${TF_BACKUP_BUCKET_NAME:-}"
          EXTRA_VARS=()

          if [[ "${{ matrix.name }}" == "live-dev" && -n "${BACKUP_BUCKET_NAME}" ]]; then
            EXTRA_VARS+=("-var=sqlite_backup_bucket_name=${BACKUP_BUCKET_NAME}")
          fi

          terraform -chdir=${{ matrix.dir }} plan -input=false -no-color -var-file=terraform.tfvars \
            "${EXTRA_VARS[@]}"

  validate-modules:
    if: ${{ github.event_name == 'pull_request' }}
    runs-on: ubuntu-latest
    needs: fmt
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Terraform validate modules
        run: |
          shopt -s nullglob
          for module in infra/aws/modules/*; do
            if ls "${module}"/*.tf >/dev/null 2>&1; then
              terraform -chdir="${module}" init -backend=false
              terraform -chdir="${module}" validate
            fi
          done

  tfsec:
    if: ${{ github.event_name == 'pull_request' }}
    runs-on: ubuntu-latest
    needs: fmt
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: tfsec scan
        uses: aquasecurity/tfsec-action@v1.0.3
        with:
          working_directory: infra/aws
          github_token: ${{ github.token }}
          additional_args: >-
            --exclude aws-s3-encryption-customer-key,aws-dynamodb-enable-at-rest-encryption,aws-dynamodb-enable-recovery,aws-s3-enable-bucket-logging,aws-dynamodb-table-customer-key

  env-select:
    if: ${{ github.event_name == 'workflow_dispatch' }}
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}
    outputs:
      tf_dir: ${{ steps.select.outputs.tf_dir }}
      tf_key: ${{ steps.select.outputs.tf_key }}
    steps:
      - name: Guard DNS_ZONE_NAME (dev only)
        run: |
          set -euo pipefail
          DNS_ZONE_NAME="${{ vars.DNS_ZONE_NAME }}"
          if [[ "${{ inputs.environment }}" == "dev" && -z "${DNS_ZONE_NAME}" ]]; then
            echo "❌ ERROR: GitHub Actions variable DNS_ZONE_NAME must be set for dev applies."
            echo "Set it in repo settings: Settings -> Secrets and variables -> Actions -> Variables."
            exit 1
          fi

      - name: Select environment
        id: select
        run: |
          case "${{ inputs.environment }}" in
            dev)
              echo "tf_dir=infra/aws/live/dev" >> "$GITHUB_OUTPUT"
              echo "tf_key=cloudradar/dev/terraform.tfstate" >> "$GITHUB_OUTPUT"
              ;;
            prod)
              echo "tf_dir=infra/aws/live/prod" >> "$GITHUB_OUTPUT"
              echo "tf_key=cloudradar/prod/terraform.tfstate" >> "$GITHUB_OUTPUT"
              ;;
          esac

      - name: env-select summary
        if: ${{ always() }}
        env:
          JOB_STATUS: ${{ job.status }}
          TARGET_ENV: ${{ inputs.environment }}
          TF_DIR: ${{ steps.select.outputs.tf_dir }}
          TF_KEY: ${{ steps.select.outputs.tf_key }}
        run: |
          {
            echo "### env-select"
            echo "- Status: \`${JOB_STATUS}\`"
            echo "- Environment: \`${TARGET_ENV}\`"
            echo "- Terraform root: \`${TF_DIR:-n/a}\`"
            echo "- Backend key: \`${TF_KEY:-n/a}\`"
          } >> "$GITHUB_STEP_SUMMARY"
          if [[ "${JOB_STATUS}" == "success" ]]; then
            echo "::notice title=env-select::${TARGET_ENV} mapped to ${TF_DIR}."
          else
            echo "::warning title=env-select::environment mapping failed."
          fi

  tf-validate:
    if: ${{ github.event_name == 'workflow_dispatch' }}
    runs-on: ubuntu-latest
    needs: env-select
    environment: ${{ inputs.environment }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Terraform init (remote backend)
        run: |
          terraform -chdir="${{ needs.env-select.outputs.tf_dir }}" init \
            -backend-config="bucket=${TF_STATE_BUCKET}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=${TF_LOCK_TABLE_NAME}" \
            -backend-config="key=${{ needs.env-select.outputs.tf_key }}"

      - name: Terraform validate
        run: terraform -chdir="${{ needs.env-select.outputs.tf_dir }}" validate

      - name: tf-validate summary
        if: ${{ always() }}
        env:
          JOB_STATUS: ${{ job.status }}
          TF_DIR: ${{ needs.env-select.outputs.tf_dir }}
        run: |
          {
            echo "### tf-validate"
            echo "- Status: \`${JOB_STATUS}\`"
            echo "- Terraform root: \`${TF_DIR}\`"
          } >> "$GITHUB_STEP_SUMMARY"
          if [[ "${JOB_STATUS}" == "success" ]]; then
            echo "::notice title=tf-validate::terraform validate passed."
          else
            echo "::warning title=tf-validate::terraform validate failed."
          fi

  orphan-scan-pre-deploy:
    if: ${{ github.event_name == 'workflow_dispatch' }}
    runs-on: ubuntu-latest
    needs: env-select
    environment: ${{ inputs.environment }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Terraform init (remote backend)
        run: |
          terraform -chdir="${{ needs.env-select.outputs.tf_dir }}" init \
            -backend-config="bucket=${TF_STATE_BUCKET}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=${TF_LOCK_TABLE_NAME}" \
            -backend-config="key=${{ needs.env-select.outputs.tf_key }}"

      - name: Orphan scan pre-deploy (strict)
        run: |
          scripts/ci/find-orphans.sh \
            --environment "${{ inputs.environment }}" \
            --tf-dir "${{ needs.env-select.outputs.tf_dir }}" \
            --mode pre-deploy \
            --strict true \
            --summary-path "$GITHUB_STEP_SUMMARY"

  tf-plan:
    if: ${{ github.event_name == 'workflow_dispatch' }}
    runs-on: ubuntu-latest
    needs:
      - env-select
      - tf-validate
      - orphan-scan-pre-deploy
    environment: ${{ inputs.environment }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Terraform init (remote backend)
        run: |
          terraform -chdir="${{ needs.env-select.outputs.tf_dir }}" init \
            -backend-config="bucket=${TF_STATE_BUCKET}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=${TF_LOCK_TABLE_NAME}" \
            -backend-config="key=${{ needs.env-select.outputs.tf_key }}"

      - name: Detach hosted zone from env state (migration, dev only)
        if: ${{ inputs.environment == 'dev' && inputs.auto_approve }}
        run: |
          set -euo pipefail
          TF_DIR="${{ needs.env-select.outputs.tf_dir }}"

          if terraform -chdir="${TF_DIR}" state list | grep -q '^aws_route53_zone\\.cloudradar\\[0\\]$'; then
            terraform -chdir="${TF_DIR}" state rm aws_route53_zone.cloudradar[0]
          fi

      - name: Terraform plan
        run: |
          set -euo pipefail
          BACKUP_BUCKET_NAME="${{ inputs.backup_bucket_name || env.TF_BACKUP_BUCKET_NAME }}"
          DNS_ZONE_NAME="${{ vars.DNS_ZONE_NAME }}"
          AIRCRAFT_REFERENCE_BUCKET_NAME="${{ vars.AIRCRAFT_REFERENCE_BUCKET_NAME || vars.TF_AIRCRAFT_REFERENCE_BUCKET_NAME }}"
          PROCESSOR_AIRCRAFT_DB_ENABLED="${{ vars.PROCESSOR_AIRCRAFT_DB_ENABLED }}"
          PROCESSOR_AIRCRAFT_DB_S3_URI="${{ vars.PROCESSOR_AIRCRAFT_DB_S3_URI }}"
          PROCESSOR_AIRCRAFT_DB_SHA256="${{ vars.PROCESSOR_AIRCRAFT_DB_SHA256 }}"
          ENABLE_EXTERNAL_ALERTING="${{ vars.ENABLE_EXTERNAL_ALERTING }}"
          ALERTS_ENABLED_VAR="${{ vars.ALERTS_ENABLED }}"
          ALERT_EMAIL_ENDPOINT="${{ vars.ALERT_EMAIL_ENDPOINT }}"
          EXTRA_VARS=()

          if [[ "${{ inputs.environment }}" == "dev" && -n "${BACKUP_BUCKET_NAME}" ]]; then
            EXTRA_VARS+=("-var=sqlite_backup_bucket_name=${BACKUP_BUCKET_NAME}")
          fi

          # Add monitoring passwords from GitHub Secrets
          if [[ -n "${{ secrets.GRAFANA_ADMIN_PASSWORD }}" ]]; then
            EXTRA_VARS+=("-var=grafana_admin_password=${{ secrets.GRAFANA_ADMIN_PASSWORD }}")
          fi

          if [[ -n "${{ secrets.PROMETHEUS_AUTH_PASSWORD }}" ]]; then
            EXTRA_VARS+=("-var=prometheus_auth_password=${{ secrets.PROMETHEUS_AUTH_PASSWORD }}")
          fi

          if [[ "${{ inputs.environment }}" == "dev" ]]; then
            if [[ -z "${DNS_ZONE_NAME}" ]]; then
              echo "❌ ERROR: DNS_ZONE_NAME is required for dev applies."
              exit 1
            fi
            EXTRA_VARS+=("-var=dns_zone_name=${DNS_ZONE_NAME}")
            if [[ -n "${AIRCRAFT_REFERENCE_BUCKET_NAME}" ]]; then
              EXTRA_VARS+=("-var=aircraft_reference_bucket_name=${AIRCRAFT_REFERENCE_BUCKET_NAME}")
            fi
          elif [[ -n "${DNS_ZONE_NAME}" ]]; then
            EXTRA_VARS+=("-var=dns_zone_name=${DNS_ZONE_NAME}")
          fi

          # Optional: processor aircraft reference DB (SSM -> ESO -> env vars)
          if [[ -n "${PROCESSOR_AIRCRAFT_DB_ENABLED}" ]]; then
            EXTRA_VARS+=("-var=processor_aircraft_db_enabled=${PROCESSOR_AIRCRAFT_DB_ENABLED}")
          fi
          if [[ -n "${PROCESSOR_AIRCRAFT_DB_S3_URI}" ]]; then
            EXTRA_VARS+=("-var=processor_aircraft_db_s3_uri=${PROCESSOR_AIRCRAFT_DB_S3_URI}")
          fi
          if [[ -n "${PROCESSOR_AIRCRAFT_DB_SHA256}" ]]; then
            EXTRA_VARS+=("-var=processor_aircraft_db_sha256=${PROCESSOR_AIRCRAFT_DB_SHA256}")
          fi

          if [[ -n "${ENABLE_EXTERNAL_ALERTING}" ]]; then
            EXTRA_VARS+=("-var=enable_external_alerting=${ENABLE_EXTERNAL_ALERTING}")
          fi
          if [[ -z "${ALERTS_ENABLED_VAR}" ]]; then
            ALERTS_ENABLED_VAR="true"
          fi
          EXTRA_VARS+=("-var=alerts_enabled=${ALERTS_ENABLED_VAR}")
          if [[ -n "${ALERT_EMAIL_ENDPOINT}" ]]; then
            EXTRA_VARS+=("-var=alert_email_endpoint=${ALERT_EMAIL_ENDPOINT}")
          fi

          terraform -chdir="${{ needs.env-select.outputs.tf_dir }}" plan -input=false -no-color -var-file=terraform.tfvars \
            "${EXTRA_VARS[@]}"

      - name: tf-plan summary
        if: ${{ always() }}
        env:
          JOB_STATUS: ${{ job.status }}
          TF_DIR: ${{ needs.env-select.outputs.tf_dir }}
          TARGET_ENV: ${{ inputs.environment }}
        run: |
          {
            echo "### tf-plan"
            echo "- Status: \`${JOB_STATUS}\`"
            echo "- Environment: \`${TARGET_ENV}\`"
            echo "- Terraform root: \`${TF_DIR}\`"
          } >> "$GITHUB_STEP_SUMMARY"
          if [[ "${JOB_STATUS}" == "success" ]]; then
            echo "::notice title=tf-plan::plan completed successfully."
          else
            echo "::warning title=tf-plan::plan failed."
          fi

  tf-apply:
    if: ${{ github.event_name == 'workflow_dispatch' }}
    runs-on: ubuntu-latest
    needs:
      - env-select
      - tf-plan
    environment: ${{ inputs.environment }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Guard apply confirmation
        if: ${{ !inputs.auto_approve }}
        run: |
          echo "Check auto_approve to proceed with apply."
          exit 1

      - name: Terraform init (remote backend)
        run: |
          terraform -chdir="${{ needs.env-select.outputs.tf_dir }}" init \
            -backend-config="bucket=${TF_STATE_BUCKET}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=${TF_LOCK_TABLE_NAME}" \
            -backend-config="key=${{ needs.env-select.outputs.tf_key }}"

      - name: Detach hosted zone from env state (migration, dev only)
        if: ${{ inputs.environment == 'dev' }}
        run: |
          set -euo pipefail
          TF_DIR="${{ needs.env-select.outputs.tf_dir }}"

          if terraform -chdir="${TF_DIR}" state list | grep -q '^aws_route53_zone\\.cloudradar\\[0\\]$'; then
            terraform -chdir="${TF_DIR}" state rm aws_route53_zone.cloudradar[0]
          fi

      - name: Terraform apply
        run: |
          set -euo pipefail
          BACKUP_BUCKET_NAME="${{ inputs.backup_bucket_name || env.TF_BACKUP_BUCKET_NAME }}"
          DNS_ZONE_NAME="${{ vars.DNS_ZONE_NAME }}"
          AIRCRAFT_REFERENCE_BUCKET_NAME="${{ vars.AIRCRAFT_REFERENCE_BUCKET_NAME || vars.TF_AIRCRAFT_REFERENCE_BUCKET_NAME }}"
          PROCESSOR_AIRCRAFT_DB_ENABLED="${{ vars.PROCESSOR_AIRCRAFT_DB_ENABLED }}"
          PROCESSOR_AIRCRAFT_DB_S3_URI="${{ vars.PROCESSOR_AIRCRAFT_DB_S3_URI }}"
          PROCESSOR_AIRCRAFT_DB_SHA256="${{ vars.PROCESSOR_AIRCRAFT_DB_SHA256 }}"
          ENABLE_EXTERNAL_ALERTING="${{ vars.ENABLE_EXTERNAL_ALERTING }}"
          ALERTS_ENABLED_VAR="${{ vars.ALERTS_ENABLED }}"
          ALERT_EMAIL_ENDPOINT="${{ vars.ALERT_EMAIL_ENDPOINT }}"
          EXTRA_VARS=()

          if [[ "${{ inputs.environment }}" == "dev" && -n "${BACKUP_BUCKET_NAME}" ]]; then
            EXTRA_VARS+=("-var=sqlite_backup_bucket_name=${BACKUP_BUCKET_NAME}")
          fi

          # Add monitoring passwords from GitHub Secrets
          if [[ -n "${{ secrets.GRAFANA_ADMIN_PASSWORD }}" ]]; then
            EXTRA_VARS+=("-var=grafana_admin_password=${{ secrets.GRAFANA_ADMIN_PASSWORD }}")
          fi

          if [[ -n "${{ secrets.PROMETHEUS_AUTH_PASSWORD }}" ]]; then
            EXTRA_VARS+=("-var=prometheus_auth_password=${{ secrets.PROMETHEUS_AUTH_PASSWORD }}")
          fi

          if [[ "${{ inputs.environment }}" == "dev" ]]; then
            if [[ -z "${DNS_ZONE_NAME}" ]]; then
              echo "❌ ERROR: DNS_ZONE_NAME is required for dev applies."
              exit 1
            fi
            EXTRA_VARS+=("-var=dns_zone_name=${DNS_ZONE_NAME}")
            if [[ -n "${AIRCRAFT_REFERENCE_BUCKET_NAME}" ]]; then
              EXTRA_VARS+=("-var=aircraft_reference_bucket_name=${AIRCRAFT_REFERENCE_BUCKET_NAME}")
            fi
          elif [[ -n "${DNS_ZONE_NAME}" ]]; then
            EXTRA_VARS+=("-var=dns_zone_name=${DNS_ZONE_NAME}")
          fi

          # Optional: processor aircraft reference DB (SSM -> ESO -> env vars)
          if [[ -n "${PROCESSOR_AIRCRAFT_DB_ENABLED}" ]]; then
            EXTRA_VARS+=("-var=processor_aircraft_db_enabled=${PROCESSOR_AIRCRAFT_DB_ENABLED}")
          fi
          if [[ -n "${PROCESSOR_AIRCRAFT_DB_S3_URI}" ]]; then
            EXTRA_VARS+=("-var=processor_aircraft_db_s3_uri=${PROCESSOR_AIRCRAFT_DB_S3_URI}")
          fi
          if [[ -n "${PROCESSOR_AIRCRAFT_DB_SHA256}" ]]; then
            EXTRA_VARS+=("-var=processor_aircraft_db_sha256=${PROCESSOR_AIRCRAFT_DB_SHA256}")
          fi

          if [[ -n "${ENABLE_EXTERNAL_ALERTING}" ]]; then
            EXTRA_VARS+=("-var=enable_external_alerting=${ENABLE_EXTERNAL_ALERTING}")
          fi
          if [[ -z "${ALERTS_ENABLED_VAR}" ]]; then
            ALERTS_ENABLED_VAR="true"
          fi
          EXTRA_VARS+=("-var=alerts_enabled=${ALERTS_ENABLED_VAR}")
          if [[ -n "${ALERT_EMAIL_ENDPOINT}" ]]; then
            EXTRA_VARS+=("-var=alert_email_endpoint=${ALERT_EMAIL_ENDPOINT}")
          fi

          terraform -chdir="${{ needs.env-select.outputs.tf_dir }}" apply -input=false -auto-approve -var-file=terraform.tfvars \
            "${EXTRA_VARS[@]}"

      - name: Re-enable CloudWatch alarm actions (best effort)
        if: ${{ vars.ALERTS_ENABLED != 'false' }}
        run: |
          set -euo pipefail
          ALARM_PREFIX="cloudradar-${{ inputs.environment }}-"
          mapfile -t alarms < <(aws cloudwatch describe-alarms \
            --query "MetricAlarms[?starts_with(AlarmName, \`${ALARM_PREFIX}\`) && contains(AlarmName, \`status-check-failed\`)].AlarmName" \
            --output text | tr '\t' '\n' | sed '/^$/d')

          if (( ${#alarms[@]} == 0 )); then
            echo "::notice title=cloudwatch-alerting::No status-check alarms found for prefix ${ALARM_PREFIX}."
            exit 0
          fi

          aws cloudwatch enable-alarm-actions --alarm-names "${alarms[@]}"
          echo "::notice title=cloudwatch-alerting::Re-enabled alarm actions for ${#alarms[@]} alarm(s)."

      - name: tf-apply summary
        if: ${{ always() }}
        env:
          JOB_STATUS: ${{ job.status }}
          TF_DIR: ${{ needs.env-select.outputs.tf_dir }}
          TARGET_ENV: ${{ inputs.environment }}
          AUTO_APPROVE: ${{ inputs.auto_approve }}
        run: |
          {
            echo "### tf-apply"
            echo "- Status: \`${JOB_STATUS}\`"
            echo "- Environment: \`${TARGET_ENV}\`"
            echo "- Auto-approve input: \`${AUTO_APPROVE}\`"
            echo "- Terraform root: \`${TF_DIR}\`"
          } >> "$GITHUB_STEP_SUMMARY"
          if [[ "${JOB_STATUS}" == "success" ]]; then
            echo "::notice title=tf-apply::apply completed."
          else
            echo "::warning title=tf-apply::apply failed or was blocked."
          fi

  alertmanager-reenable:
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.environment == 'dev' && vars.ALERTS_ENABLED != 'false' }}
    runs-on: ubuntu-latest
    needs:
      - eso-secrets-ready
      - tf-outputs
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Re-enable in-cluster Alertmanager (best effort)
        env:
          K3S_SERVER_INSTANCE_ID: ${{ needs.tf-outputs.outputs.k3s_server_instance_id }}
        run: |
          set -euo pipefail

          result="ok"
          detail="alertmanager re-enabled"

          wait_for_instance_ready() {
            local instance_id=$1
            local max_attempts=12
            local attempt=1
            local sleep_seconds=10

            while [ "${attempt}" -le "${max_attempts}" ]; do
              local state
              local ping
              state="$(aws ec2 describe-instance-status \
                --instance-ids "${instance_id}" \
                --include-all-instances \
                --query "InstanceStatuses[0].InstanceState.Name" \
                --output text 2>/dev/null || true)"

              ping="$(aws ssm describe-instance-information \
                --filters "Key=InstanceIds,Values=${instance_id}" \
                --query "InstanceInformationList[0].PingStatus" \
                --output text 2>/dev/null || true)"

              if [ "${state}" = "running" ] && [ "${ping}" = "Online" ]; then
                return 0
              fi

              sleep "${sleep_seconds}"
              attempt=$((attempt + 1))
              sleep_seconds=$((sleep_seconds + 5))
            done

            return 1
          }

          if [[ -z "${K3S_SERVER_INSTANCE_ID}" ]]; then
            result="warn"
            detail="k3s_server_instance_id missing; skipped"
          elif ! wait_for_instance_ready "${K3S_SERVER_INSTANCE_ID}"; then
            result="warn"
            detail="k3s instance not SSM-online; skipped"
          else
            params="$(jq -n \
              --arg c0 'export KUBECONFIG=/etc/rancher/k3s/k3s.yaml' \
              --arg c1 'AM_STS=""; i=0; while [ $i -lt 18 ]; do AM_STS=$(sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl -n monitoring get statefulset -l app.kubernetes.io/name=alertmanager -o jsonpath="{.items[0].metadata.name}" 2>/dev/null || true); if [ -n "$AM_STS" ]; then break; fi; echo "Waiting for Alertmanager statefulset..."; sleep 10; i=$((i+1)); done' \
              --arg c2 'if [ -z "$AM_STS" ]; then echo "Alertmanager statefulset not found in namespace monitoring after wait"; exit 2; fi' \
              --arg c3 'sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl -n monitoring scale "statefulset/$AM_STS" --replicas=1' \
              '{commands:[$c0,$c1,$c2,$c3]}')"

            unmute_cmd_id="$(aws ssm send-command \
              --instance-ids "${K3S_SERVER_INSTANCE_ID}" \
              --document-name "AWS-RunShellScript" \
              --parameters "${params}" \
              --timeout-seconds 300 \
              --query "Command.CommandId" \
              --output text 2>/dev/null || true)"

            if [[ -z "${unmute_cmd_id}" || "${unmute_cmd_id}" == "None" ]]; then
              result="warn"
              detail="send-command failed"
            elif ! aws ssm wait command-executed --command-id "${unmute_cmd_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}"; then
              aws ssm get-command-invocation --command-id "${unmute_cmd_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}" || true
              result="warn"
              detail="kubectl scale command failed"
            fi
          fi

          if [[ "${result}" == "warn" ]]; then
            echo "::warning title=alertmanager::${detail}"
          else
            echo "::notice title=alertmanager::${detail}"
          fi

          {
            echo "### alertmanager-reenable"
            echo "- Result: \`${result}\`"
            echo "- Detail: \`${detail}\`"
            echo "- Instance: \`${K3S_SERVER_INSTANCE_ID:-n/a}\`"
          } >> "$GITHUB_STEP_SUMMARY"

  setup-eso-secrets:
    name: Setup ESO Secrets (Prometheus auth)
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.environment == 'dev' }}
    runs-on: ubuntu-latest
    needs: tf-apply
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Prometheus basic auth
        run: |
          set -euo pipefail
          
          echo "Checking for /cloudradar/prometheus-password in SSM..."
          if ! PASSWORD=$(aws ssm get-parameter \
            --name /cloudradar/prometheus-password \
            --query 'Parameter.Value' \
            --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null); then
            echo "❌ ERROR: /cloudradar/prometheus-password not found in SSM"
            echo "Please create it first with: aws ssm put-parameter --name /cloudradar/prometheus-password --value '<password>' --type SecureString"
            exit 1
          fi
          
          echo "✓ Found prometheus password in SSM"
          echo "Generating bcrypt hash..."
          
          # Generate htpasswd format: admin:bcrypt_hash
          HASH=$(echo -n "$PASSWORD" | openssl passwd -apr1 -stdin)
          HTPASSWD="admin:$HASH"
          
          echo "Creating /cloudradar/prometheus-htpasswd in SSM..."
          aws ssm put-parameter \
            --name /cloudradar/prometheus-htpasswd \
            --value "$HTPASSWD" \
            --type SecureString \
            --overwrite \
            --region ${{ env.AWS_REGION }}
          
          echo "✓ Prometheus auth configured in SSM"
          echo "  - password: /cloudradar/prometheus-password"
          echo "  - htpasswd: /cloudradar/prometheus-htpasswd (admin:bcrypt_hash)"

      - name: setup-eso-secrets summary
        if: ${{ always() }}
        env:
          JOB_STATUS: ${{ job.status }}
        run: |
          {
            echo "### setup-eso-secrets"
            echo "- Status: \`${JOB_STATUS}\`"
            echo "- Parameters: \`/cloudradar/prometheus-password\`, \`/cloudradar/prometheus-htpasswd\`"
          } >> "$GITHUB_STEP_SUMMARY"
          if [[ "${JOB_STATUS}" == "success" ]]; then
            echo "::notice title=setup-eso-secrets::Prometheus auth secrets prepared."
          else
            echo "::warning title=setup-eso-secrets::failed to prepare Prometheus auth secrets."
          fi

  tf-outputs:
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.environment == 'dev' }}
    runs-on: ubuntu-latest
    needs:
      - env-select
      - tf-apply
    outputs:
      k3s_server_instance_id: ${{ steps.outputs.outputs.k3s_server_instance_id }}
      edge_instance_id: ${{ steps.outputs.outputs.edge_instance_id }}
      edge_public_ip: ${{ steps.outputs.outputs.edge_public_ip }}
      edge_basic_auth_user: ${{ steps.outputs.outputs.edge_basic_auth_user }}
      edge_basic_auth_ssm_parameter_name: ${{ steps.outputs.outputs.edge_basic_auth_ssm_parameter_name }}
      dns_zone_id: ${{ steps.outputs.outputs.dns_zone_id }}
      dns_zone_name: ${{ steps.outputs.outputs.dns_zone_name }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Terraform init (remote backend)
        run: |
          terraform -chdir="${{ needs.env-select.outputs.tf_dir }}" init \
            -backend-config="bucket=${TF_STATE_BUCKET}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=${TF_LOCK_TABLE_NAME}" \
            -backend-config="key=${{ needs.env-select.outputs.tf_key }}"

      - name: Load outputs for ArgoCD bootstrap
        id: outputs
        run: |
          outputs_json="$(terraform -chdir="${{ needs.env-select.outputs.tf_dir }}" output -json)"
          echo "k3s_server_instance_id=$(jq -r '.k3s_server_instance_id.value' <<< "${outputs_json}")" >> "$GITHUB_OUTPUT"
          echo "edge_instance_id=$(jq -r '.edge_instance_id.value' <<< "${outputs_json}")" >> "$GITHUB_OUTPUT"
          echo "edge_public_ip=$(jq -r '.edge_public_ip.value' <<< "${outputs_json}")" >> "$GITHUB_OUTPUT"
          echo "edge_basic_auth_user=$(jq -r '.edge_basic_auth_user.value' <<< "${outputs_json}")" >> "$GITHUB_OUTPUT"
          echo "edge_basic_auth_ssm_parameter_name=$(jq -r '.edge_basic_auth_ssm_parameter_name.value' <<< "${outputs_json}")" >> "$GITHUB_OUTPUT"
          echo "dns_zone_id=$(jq -r '.dns_zone_id.value // empty' <<< "${outputs_json}")" >> "$GITHUB_OUTPUT"
          echo "dns_zone_name=$(jq -r '.dns_zone_name.value // empty' <<< "${outputs_json}")" >> "$GITHUB_OUTPUT"

      - name: tf-outputs summary
        if: ${{ always() }}
        env:
          JOB_STATUS: ${{ job.status }}
          K3S_SERVER_INSTANCE_ID: ${{ steps.outputs.outputs.k3s_server_instance_id }}
          EDGE_INSTANCE_ID: ${{ steps.outputs.outputs.edge_instance_id }}
        run: |
          {
            echo "### tf-outputs"
            echo "- Status: \`${JOB_STATUS}\`"
            echo "- k3s server: \`${K3S_SERVER_INSTANCE_ID:-n/a}\`"
            echo "- edge instance: \`${EDGE_INSTANCE_ID:-n/a}\`"
          } >> "$GITHUB_STEP_SUMMARY"
          if [[ "${JOB_STATUS}" == "success" ]]; then
            echo "::notice title=tf-outputs::Terraform outputs loaded."
          else
            echo "::warning title=tf-outputs::failed to load Terraform outputs."
          fi

  k3s-ready-check:
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.environment == 'dev' }}
    runs-on: ubuntu-latest
    needs: tf-outputs
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Verify k3s readiness via SSM
        env:
          K3S_SERVER_INSTANCE_ID: ${{ needs.tf-outputs.outputs.k3s_server_instance_id }}
        run: |
          # AWS-RunShellScript executes with /bin/sh; keep SSM commands POSIX-only.
          wait_for_instance_ready() {
            local instance_id=$1
            local max_attempts=12
            local attempt=1
            local sleep_seconds=10

            while [ "${attempt}" -le "${max_attempts}" ]; do
              local state
              local ping
              state="$(aws ec2 describe-instance-status \
                --instance-ids "${instance_id}" \
                --include-all-instances \
                --query "InstanceStatuses[0].InstanceState.Name" \
                --output text 2>/dev/null || true)"

              ping="$(aws ssm describe-instance-information \
                --filters "Key=InstanceIds,Values=${instance_id}" \
                --query "InstanceInformationList[0].PingStatus" \
                --output text 2>/dev/null || true)"

              if [ "${state}" = "running" ] && [ "${ping}" = "Online" ]; then
                echo "SSM instance is online."
                return 0
              fi

              echo "Waiting for SSM instance readiness (state=${state:-unknown} ping=${ping:-unknown})..."
              sleep "${sleep_seconds}"
              attempt=$((attempt + 1))
              sleep_seconds=$((sleep_seconds + 5))
            done

            echo "SSM instance not ready after ${max_attempts} attempts."
            return 1
          }

          send_ssm_command() {
            local instance_id=$1
            local params=$2
            local timeout_seconds=$3
            local max_attempts=5
            local attempt=1
            local sleep_seconds=10

            while [ "${attempt}" -le "${max_attempts}" ]; do
              echo "SSM send-command attempt ${attempt}/${max_attempts}" >&2
              local response
              response="$(aws ssm send-command \
                --instance-ids "${instance_id}" \
                --document-name "AWS-RunShellScript" \
                --parameters "${params}" \
                --timeout-seconds "${timeout_seconds}" \
                --query "Command.CommandId" \
                --output text 2>&1 || true)"

              if echo "${response}" | grep -Eq '^[A-Fa-f0-9]{8}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{12}$'; then
                echo "${response}"
                return 0
              fi

              echo "SendCommand failed: ${response}" >&2
              if [ "${attempt}" -lt "${max_attempts}" ]; then
                echo "Retrying in ${sleep_seconds}s..." >&2
                sleep "${sleep_seconds}"
                sleep_seconds=$((sleep_seconds + 5))
              fi
              attempt=$((attempt + 1))
            done

            return 1
          }

          is_uuid() {
            echo "$1" | grep -Eq '^[A-Fa-f0-9]{8}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{12}$'
          }

          params="$(jq -cn \
            --arg c0 "i=0; while [ \$i -lt 30 ]; do if [ -x /usr/local/bin/kubectl ]; then break; fi; echo \"Waiting for kubectl...\"; sleep 10; i=\$((i+1)); done; if [ ! -x /usr/local/bin/kubectl ]; then echo \"kubectl not found after 300s\"; exit 1; fi" \
            --arg c1 "export KUBECONFIG=/etc/rancher/k3s/k3s.yaml" \
            --arg c2 "sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s wait --for=condition=Ready node --all --timeout=300s" \
            --arg c3 "sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s get nodes -o wide" \
            '{commands: [$c0, $c1, $c2, $c3]}')"

          max_attempts=5
          attempt=1
          sleep_seconds=10

          poll_ssm_command() {
            local command_id=$1
            local instance_id=$2
            local timeout_seconds=$3
            local start_ts
            start_ts="$(date +%s)"

            while true; do
              local status
              status="$(aws ssm get-command-invocation \
                --command-id "${command_id}" \
                --instance-id "${instance_id}" \
                --query "Status" \
                --output text 2>/dev/null || true)"

              case "${status}" in
                Success)
                  aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${instance_id}"
                  return 0
                  ;;
                Failed|Cancelled|TimedOut)
                  aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${instance_id}"
                  echo "SSM command status=${status}"
                  return 1
                  ;;
              esac

              if [ $(( $(date +%s) - start_ts )) -ge "${timeout_seconds}" ]; then
                echo "SSM command timed out after ${timeout_seconds}s"
                aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${instance_id}" || true
                return 1
              fi

              echo "Waiting for SSM command (status=${status:-unknown})..."
              sleep 10
            done
          }

          wait_for_instance_ready "${K3S_SERVER_INSTANCE_ID}"

          while [ "${attempt}" -le "${max_attempts}" ]; do
            echo "k3s-ready-check attempt ${attempt}/${max_attempts}"

            command_id="$(send_ssm_command "${K3S_SERVER_INSTANCE_ID}" "${params}" 600)"

            if ! is_uuid "${command_id}"; then
              echo "Invalid SSM command id: ${command_id:-empty}" >&2
              if [ "${attempt}" -lt "${max_attempts}" ]; then
                echo "Retrying in ${sleep_seconds}s..." >&2
                sleep "${sleep_seconds}"
              fi
              attempt=$((attempt + 1))
              sleep_seconds=$((sleep_seconds + 5))
              continue
            fi

            echo "::notice title=k3s-ready-check::running readiness probe (attempt ${attempt}/${max_attempts})."

            if poll_ssm_command "${command_id}" "${K3S_SERVER_INSTANCE_ID}" 600; then
              echo "k3s-ready-check succeeded"
              exit 0
            fi

            if [ "${attempt}" -lt "${max_attempts}" ]; then
              echo "Retrying in ${sleep_seconds}s..."
              sleep "${sleep_seconds}"
            fi

            attempt=$((attempt + 1))
            sleep_seconds=$((sleep_seconds + 5))
          done

          echo "k3s-ready-check failed after ${max_attempts} attempts"
          exit 1

      - name: k3s-ready-check summary
        if: ${{ always() }}
        env:
          JOB_STATUS: ${{ job.status }}
          K3S_SERVER_INSTANCE_ID: ${{ needs.tf-outputs.outputs.k3s_server_instance_id }}
        run: |
          {
            echo "### k3s-ready-check"
            echo "- Status: \`${JOB_STATUS}\`"
            echo "- Instance: \`${K3S_SERVER_INSTANCE_ID}\`"
          } >> "$GITHUB_STEP_SUMMARY"
          if [[ "${JOB_STATUS}" == "success" ]]; then
            echo "::notice title=k3s-ready-check::all nodes reported Ready."
          else
            echo "::warning title=k3s-ready-check::node readiness check failed."
          fi

  prometheus-crds:
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.environment == 'dev' }}
    runs-on: ubuntu-latest
    needs:
      - tf-outputs
      - k3s-ready-check
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Apply Prometheus CRDs before ArgoCD
        env:
          PROMETHEUS_CRD_REPO: ${{ github.repository }}
          PROMETHEUS_CRD_REVISION: ${{ github.sha }}
        run: scripts/bootstrap-prometheus-crds.sh "${{ needs.tf-outputs.outputs.k3s_server_instance_id }}" "${AWS_REGION}"

      - name: prometheus-crds summary
        if: ${{ always() }}
        env:
          JOB_STATUS: ${{ job.status }}
        run: |
          {
            echo "### prometheus-crds"
            echo "- Status: \`${JOB_STATUS}\`"
            echo "- Source: \`${{ github.repository }}@${{ github.sha }}\`"
          } >> "$GITHUB_STEP_SUMMARY"
          if [[ "${JOB_STATUS}" == "success" ]]; then
            echo "::notice title=prometheus-crds::CRDs applied."
          else
            echo "::warning title=prometheus-crds::CRD apply failed."
          fi

  argocd-install:
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.environment == 'dev' }}
    runs-on: ubuntu-latest
    needs:
      - tf-outputs
      - prometheus-crds
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Install ArgoCD via SSM
        run: scripts/bootstrap-argocd-install.sh "${{ needs.tf-outputs.outputs.k3s_server_instance_id }}" "${AWS_REGION}"

      - name: argocd-install summary
        if: ${{ always() }}
        env:
          JOB_STATUS: ${{ job.status }}
        run: |
          {
            echo "### argocd-install"
            echo "- Status: \`${JOB_STATUS}\`"
          } >> "$GITHUB_STEP_SUMMARY"
          if [[ "${JOB_STATUS}" == "success" ]]; then
            echo "::notice title=argocd-install::ArgoCD install/upgrade completed."
          else
            echo "::warning title=argocd-install::ArgoCD install/upgrade failed."
          fi

  argocd-platform:
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.environment == 'dev' }}
    runs-on: ubuntu-latest
    needs:
      - argocd-install
      - tf-outputs
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Bootstrap platform app via SSM
        env:
          ARGOCD_APP_NAME: cloudradar-platform
          ARGOCD_APP_NAMESPACE: argocd
          ARGOCD_APP_PATH: k8s/platform
        run: scripts/bootstrap-argocd-app.sh "${{ needs.tf-outputs.outputs.k3s_server_instance_id }}" "${AWS_REGION}"

      - name: argocd-platform summary
        if: ${{ always() }}
        env:
          JOB_STATUS: ${{ job.status }}
        run: |
          {
            echo "### argocd-platform"
            echo "- Status: \`${JOB_STATUS}\`"
            echo "- Application: \`cloudradar-platform\`"
          } >> "$GITHUB_STEP_SUMMARY"
          if [[ "${JOB_STATUS}" == "success" ]]; then
            echo "::notice title=argocd-platform::platform application bootstrapped."
          else
            echo "::warning title=argocd-platform::platform bootstrap failed."
          fi

  eso-ready-check:
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.environment == 'dev' }}
    runs-on: ubuntu-latest
    needs:
      - argocd-platform
      - tf-outputs
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Wait for ESO readiness via SSM
        run: scripts/bootstrap-eso-ready.sh "${{ needs.tf-outputs.outputs.k3s_server_instance_id }}" "${AWS_REGION}"

      - name: eso-ready-check summary
        if: ${{ always() }}
        env:
          JOB_STATUS: ${{ job.status }}
        run: |
          {
            echo "### eso-ready-check"
            echo "- Status: \`${JOB_STATUS}\`"
          } >> "$GITHUB_STEP_SUMMARY"
          if [[ "${JOB_STATUS}" == "success" ]]; then
            echo "::notice title=eso-ready-check::ESO controller and CRDs are ready."
          else
            echo "::warning title=eso-ready-check::ESO readiness check failed."
          fi

  argocd-apps:
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.environment == 'dev' }}
    runs-on: ubuntu-latest
    needs:
      - eso-ready-check
      - tf-outputs
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Bootstrap apps app via SSM
        env:
          IGNORE_INGESTER_REPLICAS: "true"
          ARGOCD_SERVER_SIDE_APPLY: "true"
          ARGOCD_APP_NAME: cloudradar
          ARGOCD_APP_NAMESPACE: cloudradar
          ARGOCD_APP_PATH: k8s/apps
        run: scripts/bootstrap-argocd-app.sh "${{ needs.tf-outputs.outputs.k3s_server_instance_id }}" "${AWS_REGION}"

      - name: argocd-apps summary
        if: ${{ always() }}
        env:
          JOB_STATUS: ${{ job.status }}
        run: |
          {
            echo "### argocd-apps"
            echo "- Status: \`${JOB_STATUS}\`"
            echo "- Application: \`cloudradar\`"
          } >> "$GITHUB_STEP_SUMMARY"
          if [[ "${JOB_STATUS}" == "success" ]]; then
            echo "::notice title=argocd-apps::apps application bootstrapped."
          else
            echo "::warning title=argocd-apps::apps bootstrap failed."
          fi

  eso-secrets-ready:
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.environment == 'dev' }}
    runs-on: ubuntu-latest
    needs:
      - argocd-apps
      - tf-outputs
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Wait for ExternalSecret sync via SSM
        env:
          K3S_SERVER_INSTANCE_ID: ${{ needs.tf-outputs.outputs.k3s_server_instance_id }}
        run: |
          set -euo pipefail

          wait_for_instance_ready() {
            local instance_id=$1
            local max_attempts=12
            local attempt=1
            local sleep_seconds=10

            while [ "${attempt}" -le "${max_attempts}" ]; do
              local state
              local ping
              state="$(aws ec2 describe-instance-status \
                --instance-ids "${instance_id}" \
                --include-all-instances \
                --query "InstanceStatuses[0].InstanceState.Name" \
                --output text 2>/dev/null || true)"

              ping="$(aws ssm describe-instance-information \
                --filters "Key=InstanceIds,Values=${instance_id}" \
                --query "InstanceInformationList[0].PingStatus" \
                --output text 2>/dev/null || true)"

              if [ "${state}" = "running" ] && [ "${ping}" = "Online" ]; then
                echo "SSM instance is online."
                return 0
              fi

              echo "Waiting for SSM instance readiness (state=${state:-unknown} ping=${ping:-unknown})..."
              sleep "${sleep_seconds}"
              attempt=$((attempt + 1))
              sleep_seconds=$((sleep_seconds + 5))
            done

            echo "SSM instance not ready after ${max_attempts} attempts."
            return 1
          }

          send_ssm_command() {
            local instance_id=$1
            local params=$2
            local timeout_seconds=$3
            local max_attempts=5
            local attempt=1
            local sleep_seconds=10

            while [ "${attempt}" -le "${max_attempts}" ]; do
              echo "SSM send-command attempt ${attempt}/${max_attempts}" >&2
              local response
              response="$(aws ssm send-command \
                --instance-ids "${instance_id}" \
                --document-name "AWS-RunShellScript" \
                --parameters "${params}" \
                --timeout-seconds "${timeout_seconds}" \
                --query "Command.CommandId" \
                --output text 2>&1 || true)"

              if echo "${response}" | grep -Eq '^[A-Fa-f0-9]{8}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{12}$'; then
                echo "${response}"
                return 0
              fi

              echo "SendCommand failed: ${response}" >&2
              if [ "${attempt}" -lt "${max_attempts}" ]; then
                echo "Retrying in ${sleep_seconds}s..." >&2
                sleep "${sleep_seconds}"
                sleep_seconds=$((sleep_seconds + 5))
              fi
              attempt=$((attempt + 1))
            done

            return 1
          }

          is_uuid() {
            echo "$1" | grep -Eq '^[A-Fa-f0-9]{8}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{12}$'
          }

          poll_ssm_command() {
            local command_id=$1
            local instance_id=$2
            local timeout_seconds=$3
            local start_ts
            start_ts="$(date +%s)"

            while true; do
              local status
              status="$(aws ssm get-command-invocation \
                --command-id "${command_id}" \
                --instance-id "${instance_id}" \
                --query "Status" \
                --output text 2>/dev/null || true)"

              case "${status}" in
                Success)
                  aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${instance_id}"
                  return 0
                  ;;
                Failed|Cancelled|TimedOut)
                  aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${instance_id}"
                  echo "SSM command status=${status}"
                  return 1
                  ;;
              esac

              if [ $(( $(date +%s) - start_ts )) -ge "${timeout_seconds}" ]; then
                echo "SSM command timed out after ${timeout_seconds}s"
                aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${instance_id}" || true
                return 1
              fi

              echo "Waiting for SSM command (status=${status:-unknown})..."
              sleep 10
            done
          }

          params="$(jq -cn \
            --arg c0 "set -eu" \
            --arg c1 "i=0; while [ \$i -lt 36 ]; do if [ -x /usr/local/bin/kubectl ]; then break; fi; echo \"Waiting for kubectl...\"; sleep 10; i=\$((i+1)); done; if [ ! -x /usr/local/bin/kubectl ]; then echo \"kubectl not found after 360s\"; exit 1; fi" \
            --arg c2 "export KUBECONFIG=/etc/rancher/k3s/k3s.yaml" \
            --arg c3 "max_wait=300; waited=0; while :; do count=\$(sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl get externalsecret -A --no-headers 2>/dev/null | wc -l | tr -d ' '); if [ \"\${count}\" -gt 0 ]; then echo \"Found \${count} ExternalSecret resources.\"; break; fi; if [ \"\${waited}\" -ge \"\${max_wait}\" ]; then echo \"No ExternalSecret resources found after \${max_wait}s\"; exit 1; fi; echo \"No ExternalSecret resources found yet, waiting 10s...\"; sleep 10; waited=\$((waited + 10)); done" \
            --arg c4 "sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl wait externalsecret --all --all-namespaces --for=condition=Ready --timeout=600s" \
            --arg c5 "sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl get externalsecret -A -o jsonpath='{range .items[*]}{.metadata.namespace}{\"|\"}{.metadata.name}{\"|\"}{.spec.target.name}{\"\\n\"}{end}' >/tmp/external-secrets-targets.txt" \
            --arg c6 "while IFS='|' read -r ns es target; do [ -n \"\${target}\" ] || target=\"\${es}\"; echo \"Checking secret \${ns}/\${target} (from ExternalSecret \${es})\"; sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl -n \"\${ns}\" get secret \"\${target}\" >/dev/null 2>&1 || { echo \"Missing secret \${ns}/\${target}\"; exit 1; }; done </tmp/external-secrets-targets.txt; echo \"All ExternalSecrets are Ready and target Secrets exist.\"" \
            '{commands: [$c0, $c1, $c2, $c3, $c4, $c5, $c6]}')"

          wait_for_instance_ready "${K3S_SERVER_INSTANCE_ID}"
          command_id="$(send_ssm_command "${K3S_SERVER_INSTANCE_ID}" "${params}" 900)"
          if ! is_uuid "${command_id}"; then
            echo "Invalid SSM command id: ${command_id:-empty}" >&2
            exit 1
          fi

          echo "::notice title=eso-secrets-ready::validating ExternalSecret readiness and target Secret materialization."
          poll_ssm_command "${command_id}" "${K3S_SERVER_INSTANCE_ID}" 900

      - name: eso-secrets-ready summary
        if: ${{ always() }}
        env:
          JOB_STATUS: ${{ job.status }}
        run: |
          {
            echo "### eso-secrets-ready"
            echo "- Status: \`${JOB_STATUS}\`"
          } >> "$GITHUB_STEP_SUMMARY"
          if [[ "${JOB_STATUS}" == "success" ]]; then
            echo "::notice title=eso-secrets-ready::all ExternalSecrets are ready."
          else
            echo "::warning title=eso-secrets-ready::ExternalSecret readiness gate failed."
          fi

  redis-restore:
    name: REDIS-RESTORE
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.environment == 'dev' }}
    runs-on: ubuntu-latest
    needs:
      - eso-secrets-ready
      - tf-outputs
    env:
      # Required across multiple steps; avoid "unbound variable" with `set -u`.
      K3S_SERVER_INSTANCE_ID: ${{ needs.tf-outputs.outputs.k3s_server_instance_id }}
    steps:
      - name: Decide Redis restore mode (dev only)
        id: restore_gate
        shell: bash
        env:
          BACKUP_BUCKET: ${{ env.TF_BACKUP_BUCKET_NAME }}
        run: |
          set -euo pipefail

          if [[ "${{ inputs.redis_backup_restore }}" != "true" ]]; then
            echo "::notice title=Redis restore::SKIP - input redis_backup_restore=false."
            echo "SKIP_REDIS_RESTORE=true" >> "$GITHUB_ENV"
            echo "REDIS_RESTORE_REASON=input redis_backup_restore=false" >> "$GITHUB_ENV"
            exit 0
          fi

          if [[ -z "${BACKUP_BUCKET}" ]]; then
            echo "::notice title=Redis restore::SKIP - TF_BACKUP_BUCKET_NAME not set."
            echo "SKIP_REDIS_RESTORE=true" >> "$GITHUB_ENV"
            echo "REDIS_RESTORE_REASON=TF_BACKUP_BUCKET_NAME is empty" >> "$GITHUB_ENV"
            exit 0
          fi

          echo "SKIP_REDIS_RESTORE=false" >> "$GITHUB_ENV"
          echo "REDIS_RESTORE_REASON=restore requested; evaluating backup availability and Redis freshness" >> "$GITHUB_ENV"
          echo "BACKUP_BUCKET=${BACKUP_BUCKET}" >> "$GITHUB_ENV"

      - name: Checkout
        if: ${{ env.SKIP_REDIS_RESTORE != 'true' }}
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        if: ${{ env.SKIP_REDIS_RESTORE != 'true' }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Find latest Redis backup (dev only)
        if: ${{ env.SKIP_REDIS_RESTORE != 'true' }}
        shell: bash
        run: |
          set -euo pipefail
          echo "Looking up latest Redis backup in s3://${BACKUP_BUCKET}/redis-backups/ ..."
          latest_prefix="$(aws s3api list-objects-v2 \
            --bucket "${BACKUP_BUCKET}" \
            --prefix "redis-backups/" \
            --delimiter "/" \
            --query "CommonPrefixes[].Prefix" \
            --output text | tr '\t' '\n' | sed '/^$/d' | sort | tail -n 1 || true)"

          if [[ -z "${latest_prefix}" ]]; then
            echo "::notice title=Redis restore::SKIP - no Redis backups found in s3://${BACKUP_BUCKET}/redis-backups/."
            echo "SKIP_REDIS_RESTORE=true" >> "$GITHUB_ENV"
            echo "REDIS_RESTORE_REASON=no Redis backup found in S3" >> "$GITHUB_ENV"
            exit 0
          fi

          latest_key="$(aws s3api list-objects-v2 \
            --bucket "${BACKUP_BUCKET}" \
            --prefix "${latest_prefix}" \
            --query "Contents[?ends_with(Key, \`.tgz\`)].Key | [0]" \
            --output text || true)"

          if [[ -z "${latest_key}" || "${latest_key}" == "None" ]]; then
            echo "ERROR: Could not find tgz object under prefix: ${latest_prefix}" >&2
            exit 1
          fi

          S3_URI="s3://${BACKUP_BUCKET}/${latest_key}"
          echo "Latest backup: ${S3_URI}"
          echo "S3_URI=${S3_URI}" >> "$GITHUB_ENV"

      - name: Wait for SSM instance readiness (dev only)
        if: ${{ env.SKIP_REDIS_RESTORE != 'true' }}
        shell: bash
        run: |
          set -euo pipefail
          instance_id="${K3S_SERVER_INSTANCE_ID}"
          max_attempts=12
          attempt=1
          sleep_seconds=10

          while [ "${attempt}" -le "${max_attempts}" ]; do
            state="$(aws ec2 describe-instance-status \
              --instance-ids "${instance_id}" \
              --include-all-instances \
              --query "InstanceStatuses[0].InstanceState.Name" \
              --output text 2>/dev/null || true)"

            ping="$(aws ssm describe-instance-information \
              --filters "Key=InstanceIds,Values=${instance_id}" \
              --query "InstanceInformationList[0].PingStatus" \
              --output text 2>/dev/null || true)"

            if [ "${state}" = "running" ] && [ "${ping}" = "Online" ]; then
              echo "SSM instance is online."
              exit 0
            fi

            echo "Waiting for SSM instance readiness (state=${state:-unknown} ping=${ping:-unknown})..."
            sleep "${sleep_seconds}"
            attempt=$((attempt + 1))
            sleep_seconds=$((sleep_seconds + 5))
          done

          echo "SSM instance not ready after ${max_attempts} attempts."
          exit 1

      - name: "SSM: wait Redis Ready (dev only)"
        if: ${{ env.SKIP_REDIS_RESTORE != 'true' }}
        shell: bash
        run: |
          set -euo pipefail
          params="$(jq -n \
            --arg c0 'i=0; while [ $i -lt 30 ]; do if [ -x /usr/local/bin/kubectl ]; then break; fi; echo "Waiting for kubectl..."; sleep 10; i=$((i+1)); done; if [ ! -x /usr/local/bin/kubectl ]; then echo "kubectl not found after 300s"; exit 1; fi' \
            --arg c1 "export KUBECONFIG=/etc/rancher/k3s/k3s.yaml" \
            --arg c2 "sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl -n data rollout status statefulset/redis --timeout=300s" \
            '{commands:[$c0,$c1,$c2]}')"

          command_id="$(aws ssm send-command \
            --instance-ids "${K3S_SERVER_INSTANCE_ID}" \
            --document-name "AWS-RunShellScript" \
            --parameters "${params}" \
            --timeout-seconds 600 \
            --query "Command.CommandId" \
            --output text)"

          echo "::notice title=Redis restore::waiting for Redis StatefulSet rollout."
          if ! aws ssm wait command-executed --command-id "${command_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}"; then
            aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}" || true
            exit 1
          fi

      - name: "SSM: check Redis /data emptiness (dev only)"
        if: ${{ env.SKIP_REDIS_RESTORE != 'true' }}
        id: redis_fresh
        shell: bash
        run: |
          set -euo pipefail
          params="$(jq -n \
            --arg c0 "export KUBECONFIG=/etc/rancher/k3s/k3s.yaml" \
            --arg c1 'if sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl -n data exec statefulset/redis -c redis -- sh -lc "test -z \"$(ls -A /data 2>/dev/null)\""; then echo "fresh=true"; else echo "fresh=false"; fi' \
            '{commands:[$c0,$c1]}')"

          command_id="$(aws ssm send-command \
            --instance-ids "${K3S_SERVER_INSTANCE_ID}" \
            --document-name "AWS-RunShellScript" \
            --parameters "${params}" \
            --timeout-seconds 120 \
            --query "Command.CommandId" \
            --output text)"

          aws ssm wait command-executed --command-id "${command_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}"
          out="$(aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}" --query "StandardOutputContent" --output text || true)"
          echo "SSM output: ${out}"
          if echo "${out}" | grep -q "fresh=true"; then
            echo "fresh=true" >> "$GITHUB_OUTPUT"
          else
            echo "fresh=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Skip restore (Redis not fresh) (dev only)
        if: ${{ env.SKIP_REDIS_RESTORE != 'true' && steps.redis_fresh.outputs.fresh != 'true' }}
        run: |
          echo "::notice title=Redis restore::SKIP - Redis /data is not empty (protect existing data)."
          echo "REDIS_RESTORE_REASON=Redis /data is not empty" >> "$GITHUB_ENV"

      - name: "SSM: restore Redis from S3 (dev only)"
        if: ${{ env.SKIP_REDIS_RESTORE != 'true' && steps.redis_fresh.outputs.fresh == 'true' }}
        shell: bash
        run: |
          set -euo pipefail
          RESTORE_B64="$(base64 -w0 scripts/redis-restore.sh)"
          params="$(jq -n \
            --arg c0 'i=0; while [ $i -lt 30 ]; do if [ -x /usr/local/bin/kubectl ]; then break; fi; echo "Waiting for kubectl..."; sleep 10; i=$((i+1)); done; if [ ! -x /usr/local/bin/kubectl ]; then echo "kubectl not found after 300s"; exit 1; fi' \
            --arg c1 "export KUBECONFIG=/etc/rancher/k3s/k3s.yaml" \
            --arg c2 "echo '${RESTORE_B64}' | base64 -d > /tmp/redis-restore.sh && chmod +x /tmp/redis-restore.sh" \
            --arg c3 "/tmp/redis-restore.sh --force --s3-uri ${S3_URI} --kubeconfig /etc/rancher/k3s/k3s.yaml --kubectl /usr/local/bin/kubectl --region ${AWS_REGION}" \
            '{commands:[$c0,$c1,$c2,$c3]}')"

          command_id="$(aws ssm send-command \
            --instance-ids "${K3S_SERVER_INSTANCE_ID}" \
            --document-name "AWS-RunShellScript" \
            --parameters "${params}" \
            --timeout-seconds 900 \
            --query "Command.CommandId" \
            --output text)"

          echo "::notice title=Redis restore::executing restore script from latest backup."
          if ! aws ssm wait command-executed --command-id "${command_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}"; then
            aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}" || true
            exit 1
          fi
          echo "REDIS_RESTORE_EXECUTED=true" >> "$GITHUB_ENV"

      - name: Redis restore verdict (dev only)
        if: ${{ always() }}
        shell: bash
        run: |
          set -euo pipefail

          verdict="SKIPPED"
          reason="${REDIS_RESTORE_REASON:-unspecified}"

          if [[ "${SKIP_REDIS_RESTORE:-false}" != "true" ]] && [[ "${REDIS_RESTORE_EXECUTED:-false}" == "true" ]]; then
            verdict="RUN"
            reason="restored from ${S3_URI:-unknown}"
          fi

          echo "::notice title=Redis restore verdict::${verdict} - ${reason}"
          {
            echo "### Redis restore verdict"
            echo ""
            echo "- Verdict: \`${verdict}\`"
            echo "- Reason: ${reason}"
            echo "- Requested input: \`${{ inputs.redis_backup_restore }}\`"
            echo "- Backup bucket: \`${BACKUP_BUCKET:-n/a}\`"
            echo "- Backup artifact: \`${S3_URI:-n/a}\`"
          } >> "$GITHUB_STEP_SUMMARY"

  smoke-tests:
    name: SMOKE-TESTS
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.environment == 'dev' }}
    runs-on: ubuntu-latest
    needs:
      - eso-secrets-ready
      - redis-restore
      - tf-outputs
    steps:
      - name: Decide smoke-tests mode (dev only)
        id: smoke_gate
        shell: bash
        run: |
          set -euo pipefail

          if [[ "${{ inputs.run_smoke_tests }}" != "true" ]]; then
            echo "::notice title=Smoke tests::SKIP - input run_smoke_tests=false."
            echo "SKIP_SMOKE_TESTS=true" >> "$GITHUB_ENV"
            echo "SMOKE_TESTS_REASON=input run_smoke_tests=false" >> "$GITHUB_ENV"
            exit 0
          fi

          echo "SKIP_SMOKE_TESTS=false" >> "$GITHUB_ENV"
          echo "SMOKE_TESTS_REASON=enabled; running post-apply smoke checks" >> "$GITHUB_ENV"

      - name: Configure AWS credentials
        if: ${{ env.SKIP_SMOKE_TESTS != 'true' }}
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Wait for ArgoCD app sync
        if: ${{ env.SKIP_SMOKE_TESTS != 'true' }}
        env:
          K3S_SERVER_INSTANCE_ID: ${{ needs.tf-outputs.outputs.k3s_server_instance_id }}
        run: |
          wait_for_instance_ready() {
            local instance_id=$1
            local max_attempts=12
            local attempt=1
            local sleep_seconds=10

            while [ "${attempt}" -le "${max_attempts}" ]; do
              local state
              local ping
              state="$(aws ec2 describe-instance-status \
                --instance-ids "${instance_id}" \
                --include-all-instances \
                --query "InstanceStatuses[0].InstanceState.Name" \
                --output text 2>/dev/null || true)"

              ping="$(aws ssm describe-instance-information \
                --filters "Key=InstanceIds,Values=${instance_id}" \
                --query "InstanceInformationList[0].PingStatus" \
                --output text 2>/dev/null || true)"

              if [ "${state}" = "running" ] && [ "${ping}" = "Online" ]; then
                echo "SSM instance is online."
                return 0
              fi

              echo "Waiting for SSM instance readiness (state=${state:-unknown} ping=${ping:-unknown})..."
              sleep "${sleep_seconds}"
              attempt=$((attempt + 1))
              sleep_seconds=$((sleep_seconds + 5))
            done

            echo "SSM instance not ready after ${max_attempts} attempts."
            return 1
          }

          send_ssm_command() {
            local instance_id=$1
            local params=$2
            local timeout_seconds=$3
            local max_attempts=5
            local attempt=1
            local sleep_seconds=10

            while [ "${attempt}" -le "${max_attempts}" ]; do
              echo "SSM send-command attempt ${attempt}/${max_attempts}" >&2
              local response
              response="$(aws ssm send-command \
                --instance-ids "${instance_id}" \
                --document-name "AWS-RunShellScript" \
                --parameters "${params}" \
                --timeout-seconds "${timeout_seconds}" \
                --query "Command.CommandId" \
                --output text 2>&1 || true)"

              if echo "${response}" | grep -Eq '^[A-Fa-f0-9]{8}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{12}$'; then
                echo "${response}"
                return 0
              fi

              echo "SendCommand failed: ${response}" >&2
              if [ "${attempt}" -lt "${max_attempts}" ]; then
                echo "Retrying in ${sleep_seconds}s..." >&2
                sleep "${sleep_seconds}"
                sleep_seconds=$((sleep_seconds + 5))
              fi
              attempt=$((attempt + 1))
            done

            return 1
          }

          is_uuid() {
            echo "$1" | grep -Eq '^[A-Fa-f0-9]{8}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{12}$'
          }

          params="$(jq -cn \
            --arg c0 "i=0; while [ \$i -lt 30 ]; do if [ -x /usr/local/bin/kubectl ]; then break; fi; echo \"Waiting for kubectl...\"; sleep 10; i=\$((i+1)); done; if [ ! -x /usr/local/bin/kubectl ]; then echo \"kubectl not found after 300s\"; exit 1; fi" \
            --arg c1 "export KUBECONFIG=/etc/rancher/k3s/k3s.yaml" \
            --arg c2 "set -eu" \
            --arg c3 "i=0; while [ \$i -lt 30 ]; do sync=\$(sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n argocd get application cloudradar -o jsonpath={.status.sync.status} 2>/dev/null || true); health=\$(sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n argocd get application cloudradar -o jsonpath={.status.health.status} 2>/dev/null || true); if [ \"\$sync\" = \"Synced\" ] && [ \"\$health\" = \"Healthy\" ]; then echo \"ArgoCD app is synced and healthy\"; exit 0; fi; echo \"Waiting for ArgoCD app (sync=\$sync health=\$health)\"; if [ \$((i % 3)) -eq 0 ]; then echo \"---\"; echo \"Pods cloudradar:\"; sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n cloudradar get pods -o wide || true; echo \"Pods monitoring:\"; sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n monitoring get pods -o wide || true; fi; sleep 10; i=\$((i+1)); done; echo \"ArgoCD app not ready\"; exit 1" \
            '{commands: [$c0, $c1, $c2, $c3]}')"

          poll_ssm_command() {
            local command_id=$1
            local instance_id=$2
            local timeout_seconds=$3
            local start_ts
            start_ts="$(date +%s)"

            while true; do
              local status
              status="$(aws ssm get-command-invocation \
                --command-id "${command_id}" \
                --instance-id "${instance_id}" \
                --query "Status" \
                --output text 2>/dev/null || true)"

              case "${status}" in
                Success)
                  aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${instance_id}"
                  return 0
                  ;;
                Failed|Cancelled|TimedOut)
                  aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${instance_id}"
                  echo "SSM command status=${status}"
                  return 1
                  ;;
              esac

              if [ $(( $(date +%s) - start_ts )) -ge "${timeout_seconds}" ]; then
                echo "SSM command timed out after ${timeout_seconds}s"
                aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${instance_id}" || true
                return 1
              fi

              echo "Waiting for SSM command (status=${status:-unknown})..."
              sleep 10
            done
          }

          wait_for_instance_ready "${K3S_SERVER_INSTANCE_ID}"
          command_id="$(send_ssm_command "${K3S_SERVER_INSTANCE_ID}" "${params}" 600)"

          if ! is_uuid "${command_id}"; then
            echo "Invalid SSM command id: ${command_id:-empty}" >&2
            exit 1
          fi

          echo "::notice title=SMOKE-TESTS::checking ArgoCD app sync/health."

          poll_ssm_command "${command_id}" "${K3S_SERVER_INSTANCE_ID}" 600

      - name: Wait for healthz deployment
        if: ${{ env.SKIP_SMOKE_TESTS != 'true' }}
        env:
          K3S_SERVER_INSTANCE_ID: ${{ needs.tf-outputs.outputs.k3s_server_instance_id }}
          EDGE_PUBLIC_IP: ${{ needs.tf-outputs.outputs.edge_public_ip }}
          EDGE_BASIC_AUTH_USER: ${{ needs.tf-outputs.outputs.edge_basic_auth_user }}
          EDGE_BASIC_AUTH_SSM_PARAMETER_NAME: ${{ needs.tf-outputs.outputs.edge_basic_auth_ssm_parameter_name }}
        run: |
          wait_for_instance_ready() {
            local instance_id=$1
            local max_attempts=12
            local attempt=1
            local sleep_seconds=10

            while [ "${attempt}" -le "${max_attempts}" ]; do
              local state
              local ping
              state="$(aws ec2 describe-instance-status \
                --instance-ids "${instance_id}" \
                --include-all-instances \
                --query "InstanceStatuses[0].InstanceState.Name" \
                --output text 2>/dev/null || true)"

              ping="$(aws ssm describe-instance-information \
                --filters "Key=InstanceIds,Values=${instance_id}" \
                --query "InstanceInformationList[0].PingStatus" \
                --output text 2>/dev/null || true)"

              if [ "${state}" = "running" ] && [ "${ping}" = "Online" ]; then
                echo "SSM instance is online."
                return 0
              fi

              echo "Waiting for SSM instance readiness (state=${state:-unknown} ping=${ping:-unknown})..."
              sleep "${sleep_seconds}"
              attempt=$((attempt + 1))
              sleep_seconds=$((sleep_seconds + 5))
            done

            echo "SSM instance not ready after ${max_attempts} attempts."
            return 1
          }

          send_ssm_command() {
            local instance_id=$1
            local params=$2
            local timeout_seconds=$3
            local max_attempts=5
            local attempt=1
            local sleep_seconds=10

            while [ "${attempt}" -le "${max_attempts}" ]; do
              echo "SSM send-command attempt ${attempt}/${max_attempts}" >&2
              local response
              response="$(aws ssm send-command \
                --instance-ids "${instance_id}" \
                --document-name "AWS-RunShellScript" \
                --parameters "${params}" \
                --timeout-seconds "${timeout_seconds}" \
                --query "Command.CommandId" \
                --output text 2>&1 || true)"

              if echo "${response}" | grep -Eq '^[A-Fa-f0-9]{8}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{12}$'; then
                echo "${response}"
                return 0
              fi

              echo "SendCommand failed: ${response}" >&2
              if [ "${attempt}" -lt "${max_attempts}" ]; then
                echo "Retrying in ${sleep_seconds}s..." >&2
                sleep "${sleep_seconds}"
                sleep_seconds=$((sleep_seconds + 5))
              fi
              attempt=$((attempt + 1))
            done

            return 1
          }

          is_uuid() {
            echo "$1" | grep -Eq '^[A-Fa-f0-9]{8}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{12}$'
          }

          params="$(jq -cn \
            --arg c0 "i=0; while [ \$i -lt 30 ]; do if [ -x /usr/local/bin/kubectl ]; then break; fi; echo \"Waiting for kubectl...\"; sleep 10; i=\$((i+1)); done; if [ ! -x /usr/local/bin/kubectl ]; then echo \"kubectl not found after 300s\"; exit 1; fi" \
            --arg c1 "export KUBECONFIG=/etc/rancher/k3s/k3s.yaml" \
            --arg c2 "sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n cloudradar rollout status deploy/healthz --timeout=300s" \
            --arg c3 "sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n cloudradar get deploy/healthz" \
            '{commands: [$c0, $c1, $c2, $c3]}')"

          poll_ssm_command() {
            local command_id=$1
            local instance_id=$2
            local timeout_seconds=$3
            local start_ts
            start_ts="$(date +%s)"

            while true; do
              local status
              status="$(aws ssm get-command-invocation \
                --command-id "${command_id}" \
                --instance-id "${instance_id}" \
                --query "Status" \
                --output text 2>/dev/null || true)"

              case "${status}" in
                Success)
                  aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${instance_id}"
                  return 0
                  ;;
                Failed|Cancelled|TimedOut)
                  aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${instance_id}"
                  echo "SSM command status=${status}"
                  return 1
                  ;;
              esac

              if [ $(( $(date +%s) - start_ts )) -ge "${timeout_seconds}" ]; then
                echo "SSM command timed out after ${timeout_seconds}s"
                aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${instance_id}" || true
                return 1
              fi

              echo "Waiting for SSM command (status=${status:-unknown})..."
              sleep 10
            done
          }

          wait_for_instance_ready "${K3S_SERVER_INSTANCE_ID}"
          command_id="$(send_ssm_command "${K3S_SERVER_INSTANCE_ID}" "${params}" 600)"

          if ! is_uuid "${command_id}"; then
            echo "Invalid SSM command id: ${command_id:-empty}" >&2
            exit 1
          fi

          echo "::notice title=SMOKE-TESTS::checking healthz deployment rollout."

          poll_ssm_command "${command_id}" "${K3S_SERVER_INSTANCE_ID}" 600

      - name: Verify /healthz from the Internet
        if: ${{ env.SKIP_SMOKE_TESTS != 'true' }}
        env:
          EDGE_INSTANCE_ID: ${{ needs.tf-outputs.outputs.edge_instance_id }}
          EDGE_PUBLIC_IP: ${{ needs.tf-outputs.outputs.edge_public_ip }}
          EDGE_BASIC_AUTH_USER: ${{ needs.tf-outputs.outputs.edge_basic_auth_user }}
          EDGE_BASIC_AUTH_SSM_PARAMETER_NAME: ${{ needs.tf-outputs.outputs.edge_basic_auth_ssm_parameter_name }}
          K3S_SERVER_INSTANCE_ID: ${{ needs.tf-outputs.outputs.k3s_server_instance_id }}
        run: |
          wait_for_instance_ready() {
            local instance_id=$1
            local max_attempts=12
            local attempt=1
            local sleep_seconds=10

            while [ "${attempt}" -le "${max_attempts}" ]; do
              local state
              local ping
              state="$(aws ec2 describe-instance-status \
                --instance-ids "${instance_id}" \
                --include-all-instances \
                --query "InstanceStatuses[0].InstanceState.Name" \
                --output text 2>/dev/null || true)"

              ping="$(aws ssm describe-instance-information \
                --filters "Key=InstanceIds,Values=${instance_id}" \
                --query "InstanceInformationList[0].PingStatus" \
                --output text 2>/dev/null || true)"

              if [ "${state}" = "running" ] && [ "${ping}" = "Online" ]; then
                echo "SSM instance is online."
                return 0
              fi

              echo "Waiting for SSM instance readiness (state=${state:-unknown} ping=${ping:-unknown})..."
              sleep "${sleep_seconds}"
              attempt=$((attempt + 1))
              sleep_seconds=$((sleep_seconds + 5))
            done

            echo "SSM instance not ready after ${max_attempts} attempts."
            return 1
          }

          send_ssm_command() {
            local instance_id=$1
            local params=$2
            local timeout_seconds=$3
            local max_attempts=5
            local attempt=1
            local sleep_seconds=10

            while [ "${attempt}" -le "${max_attempts}" ]; do
              echo "SSM send-command attempt ${attempt}/${max_attempts}" >&2
              local response
              response="$(aws ssm send-command \
                --instance-ids "${instance_id}" \
                --document-name "AWS-RunShellScript" \
                --parameters "${params}" \
                --timeout-seconds "${timeout_seconds}" \
                --query "Command.CommandId" \
                --output text 2>&1 || true)"

              if echo "${response}" | grep -Eq '^[A-Fa-f0-9]{8}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{12}$'; then
                echo "${response}"
                return 0
              fi

              echo "SendCommand failed: ${response}" >&2
              if [ "${attempt}" -lt "${max_attempts}" ]; then
                echo "Retrying in ${sleep_seconds}s..." >&2
                sleep "${sleep_seconds}"
                sleep_seconds=$((sleep_seconds + 5))
              fi
              attempt=$((attempt + 1))
            done

            return 1
          }

          is_uuid() {
            echo "$1" | grep -Eq '^[A-Fa-f0-9]{8}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{4}-[A-Fa-f0-9]{12}$'
          }

          params="$(jq -cn \
            --arg c0 'set -euo pipefail' \
            --arg c1 'nginx_status=$(systemctl is-active nginx || true); echo "nginx_status=${nginx_status}"' \
            --arg c2 'ss -ltnp | grep -E ":443" || true' \
            --arg c3 'if [ "${nginx_status}" != "active" ]; then systemctl status nginx --no-pager -l; journalctl -u nginx --no-pager -n 200 || true; exit 1; fi' \
            --arg c4 'ss -ltnp | grep -E ":443" >/dev/null' \
            '{commands: [$c0, $c1, $c2, $c3, $c4]}')"

          poll_ssm_command() {
            local command_id=$1
            local instance_id=$2
            local timeout_seconds=$3
            local start_ts
            start_ts="$(date +%s)"

            while true; do
              local status
              status="$(aws ssm get-command-invocation \
                --command-id "${command_id}" \
                --instance-id "${instance_id}" \
                --query "Status" \
                --output text 2>/dev/null || true)"

              case "${status}" in
                Success)
                  aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${instance_id}"
                  return 0
                  ;;
                Failed|Cancelled|TimedOut)
                  aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${instance_id}"
                  echo "SSM command status=${status}"
                  return 1
                  ;;
              esac

              if [ $(( $(date +%s) - start_ts )) -ge "${timeout_seconds}" ]; then
                echo "SSM command timed out after ${timeout_seconds}s"
                aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${instance_id}" || true
                return 1
              fi

              echo "Waiting for SSM command (status=${status:-unknown})..."
              sleep 10
            done
          }

          run_cluster_diagnostics() {
            local failed_path=$1

            if [ -z "${K3S_SERVER_INSTANCE_ID:-}" ]; then
              echo "K3S_SERVER_INSTANCE_ID not set; skipping diagnostics for ${failed_path}."
              return 0
            fi

            echo "---"
            echo "Running k3s diagnostics for ${failed_path}..."

            set +e
            trap 'set -e' RETURN

            if ! wait_for_instance_ready "${K3S_SERVER_INSTANCE_ID}"; then
              echo "K3S instance not ready; skipping diagnostics."
              return 0
            fi

            local diag_params
            diag_params="$(jq -cn \
              --arg c0 "i=0; while [ \$i -lt 30 ]; do if [ -x /usr/local/bin/kubectl ]; then break; fi; echo \"Waiting for kubectl...\"; sleep 10; i=\$((i+1)); done; if [ ! -x /usr/local/bin/kubectl ]; then echo \"kubectl not found after 300s\"; exit 1; fi" \
              --arg c1 "export KUBECONFIG=/etc/rancher/k3s/k3s.yaml" \
              --arg c2 "set -e" \
              --arg c3 "echo \"ArgoCD apps:\"; sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n argocd get application cloudradar prometheus -o wide || true" \
              --arg c4 "echo \"ArgoCD app details (cloudradar):\"; sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n argocd get application cloudradar -o yaml | grep -nE \"sync|health|message|reason|status\" || true" \
              --arg c5 "echo \"ArgoCD app details (prometheus):\"; sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n argocd get application prometheus -o yaml | grep -nE \"sync|health|message|reason|status\" || true" \
              --arg c6 "echo \"Nodes:\"; sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s get nodes -o wide || true" \
              --arg c7 "echo \"Pods cloudradar:\"; sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n cloudradar get pods -o wide || true" \
              --arg c8 "echo \"Pods monitoring:\"; sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n monitoring get pods -o wide || true" \
              --arg c9 "echo \"StatefulSets monitoring:\"; sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n monitoring get sts -o wide || true" \
              --arg c10 "echo \"PVCs monitoring:\"; sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n monitoring get pvc -o wide || true" \
              --arg c11 "echo \"Prometheus CRs:\"; sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n monitoring get prometheus -o wide || true" \
              --arg c12 "echo \"Services monitoring:\"; sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n monitoring get svc grafana prometheus-kube-prometheus-prometheus -o wide || true" \
              --arg c13 "echo \"Endpoints monitoring:\"; sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n monitoring get endpoints grafana prometheus-kube-prometheus-prometheus -o wide || true" \
              --arg c14 "echo \"Ingress monitoring:\"; sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n monitoring get ingress grafana prometheus -o wide || true" \
              --arg c15 "echo \"Services cloudradar:\"; sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n cloudradar get svc healthz -o wide || true" \
              --arg c16 "echo \"Endpoints cloudradar:\"; sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n cloudradar get endpoints healthz -o wide || true" \
              --arg c17 "echo \"Recent events monitoring:\"; sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n monitoring get events --sort-by=.lastTimestamp | tail -n 30 || true" \
              --arg c18 "echo \"Recent events cloudradar:\"; sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n cloudradar get events --sort-by=.lastTimestamp | tail -n 30 || true" \
              --arg c19 "echo \"Traefik NodePort checks (localhost:30080):\"; curl -sS -o /dev/null -w \"grafana=http %{http_code}\\n\" -H \"Host: grafana.cloudradar.local\" http://127.0.0.1:30080/ || true; curl -sS -o /dev/null -w \"prometheus=http %{http_code}\\n\" -H \"Host: prometheus.cloudradar.local\" http://127.0.0.1:30080/ || true" \
              --arg c20 "health_nodeport=\"\$(sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl --request-timeout=5s -n cloudradar get svc healthz -o jsonpath='{.spec.ports[0].nodePort}' 2>/dev/null || true)\"; if [ -n \"\${health_nodeport}\" ]; then echo \"Healthz NodePort check (localhost:\${health_nodeport}):\"; curl -sS -o /dev/null -w \"healthz=http %{http_code}\\n\" \"http://127.0.0.1:\${health_nodeport}/healthz\" || true; else echo \"Healthz NodePort not found\"; fi" \
              '{commands: [$c0, $c1, $c2, $c3, $c4, $c5, $c6, $c7, $c8, $c9, $c10, $c11, $c12, $c13, $c14, $c15, $c16, $c17, $c18, $c19, $c20]}')"

            local diag_command_id
            diag_command_id="$(send_ssm_command "${K3S_SERVER_INSTANCE_ID}" "${diag_params}" 300)"
            if ! is_uuid "${diag_command_id}"; then
              echo "Invalid SSM command id for diagnostics: ${diag_command_id:-empty}" >&2
              return 0
            fi

            echo "::notice title=SMOKE-TESTS::collecting k3s diagnostics for ${failed_path}."
            poll_ssm_command "${diag_command_id}" "${K3S_SERVER_INSTANCE_ID}" 300 || true
          }

          wait_for_instance_ready "${EDGE_INSTANCE_ID}"
          i=0
          while [ $i -lt 3 ]; do
            command_id="$(send_ssm_command "${EDGE_INSTANCE_ID}" "${params}" 120)"

            if ! is_uuid "${command_id}"; then
              echo "Invalid SSM command id: ${command_id:-empty}" >&2
              i=$((i+1))
              if [ $i -lt 3 ]; then
                echo "Edge nginx check retrying in 10s..." >&2
                sleep 10
                continue
              fi
              echo "Edge nginx check failed due to invalid command id." >&2
              exit 1
            fi

            echo "::notice title=SMOKE-TESTS::edge nginx check attempt $((i+1))/3."

            if poll_ssm_command "${command_id}" "${EDGE_INSTANCE_ID}" 120; then
              break
            fi

            i=$((i+1))
            if [ $i -lt 3 ]; then
              echo "Edge nginx not ready, retrying in 10s..."
              sleep 10
            else
              echo "Edge nginx not ready after 3 attempts."
              exit 1
            fi
          done

          edge_basic_auth_password="$(aws ssm get-parameter \
            --name "${EDGE_BASIC_AUTH_SSM_PARAMETER_NAME}" \
            --with-decryption \
            --query "Parameter.Value" \
            --output text)"

          echo "::add-mask::${edge_basic_auth_password}"

          check_edge_path() {
            local path=$1
            local max_attempts=${2:-6}
            local attempt=1
            local sleep_seconds=10
            local status_code=""
            local curl_exit=0

            while [ "${attempt}" -le "${max_attempts}" ]; do
              set +e
              status_code="$(curl -k -sS -o /dev/null -w '%{http_code}' \
                --connect-timeout 5 \
                --max-time 10 \
                --retry 2 \
                --retry-delay 2 \
                --retry-all-errors \
                -u "${EDGE_BASIC_AUTH_USER}:${edge_basic_auth_password}" \
                "https://${EDGE_PUBLIC_IP}${path}")"
              curl_exit=$?
              set -e

              if [ "${curl_exit}" -ne 0 ]; then
                echo "curl_exit=${curl_exit} for ${path} (attempt ${attempt}/${max_attempts})"
                status_code="${status_code:-000}"
              fi

              echo "HTTP ${status_code} for ${path} (attempt ${attempt}/${max_attempts}, curl_exit=${curl_exit})"

              case "${status_code}" in
                200|301|302)
                  return 0
                  ;;
              esac

              sleep "${sleep_seconds}"
              attempt=$((attempt + 1))
            done

            echo "Expected 200/301/302 from ${path}, got ${status_code}."
            run_cluster_diagnostics "${path}"
            return 1
          }

          check_edge_path "/healthz" 3
          check_edge_path "/grafana/" 6
          check_edge_path "/prometheus/" 6

      - name: Smoke-tests verdict (dev only)
        if: ${{ always() }}
        shell: bash
        env:
          JOB_STATUS: ${{ job.status }}
        run: |
          set -euo pipefail

          verdict="SKIPPED"
          reason="${SMOKE_TESTS_REASON:-unspecified}"
          if [[ "${SKIP_SMOKE_TESTS:-false}" != "true" ]]; then
            if [[ "${JOB_STATUS:-success}" == "success" ]]; then
              verdict="RUN"
              reason="completed smoke checks"
            else
              verdict="FAILED"
              if [[ "${reason}" == "unspecified" ]]; then
                reason="smoke checks did not complete successfully (job status: ${JOB_STATUS:-unknown})"
              fi
            fi
          fi

          echo "::notice title=Smoke-tests verdict::${verdict} - ${reason}"
          {
            echo "### Smoke-tests verdict"
            echo ""
            echo "- Verdict: \`${verdict}\`"
            echo "- Reason: ${reason}"
            echo "- Requested input: \`${{ inputs.run_smoke_tests }}\`"
          } >> "$GITHUB_STEP_SUMMARY"
