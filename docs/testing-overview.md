# CloudRadar â€” Testing & Quality Assurance Overview

> Vue consolidÃ©e de la stratÃ©gie de tests, des pipelines de qualitÃ© et des pratiques DevSecOps du projet CloudRadar.
> Objectif : montrer une approche structurÃ©e, multi-couche, conforme aux bonnes pratiques Cloud Architecture et DevSecOps.

---

## Table des matiÃ¨res

- [CloudRadar â€” Testing \& Quality Assurance Overview](#cloudradar--testing--quality-assurance-overview)
  - [Table des matiÃ¨res](#table-des-matiÃ¨res)
  - [1. Big Picture](#1-big-picture)
    - [Chiffres clÃ©s](#chiffres-clÃ©s)
  - [2. Shift-Left Testing](#2-shift-left-testing)
  - [3. Les 9 catÃ©gories de tests](#3-les-9-catÃ©gories-de-tests)
    - [Matrice catÃ©gorie Ã— couverture](#matrice-catÃ©gorie--couverture)
  - [4. Pipelines CI/CD](#4-pipelines-cicd)
  - [5. Couverture par service](#5-couverture-par-service)
  - [6. AmÃ©liorations possibles](#6-amÃ©liorations-possibles)

---

## 1. Big Picture

CloudRadar est composÃ© de **6 microservices** Ã©crits en 4 langages (Java, TypeScript, Python, Shell), communiquant via **Redis** comme bus de donnÃ©es. Cette diversitÃ© technique impose une stratÃ©gie de tests multi-couche : on ne peut pas tout couvrir avec un seul framework. La pyramide ci-dessous montre comment les tests sont empilÃ©s, du plus rapide (unitaire) au plus lent (performance).

```mermaid
block-beta
  columns 2

  P["ğŸ”ï¸ Performance<br>tenue en charge"]:1 Pd["k6 nightly Â· 10 VUs Â· p95 < 1500ms"]:1
  E["ğŸŒ E2E Smoke<br>santÃ© aprÃ¨s dÃ©ploiement"]:1 Ed["/healthz Â· /api/flights Â· /grafana"]:1
  D["ğŸ”— Integration<br>services + vraie base Redis"]:1 Dd["Redis Testcontainers Â· 3 services"]:1
  C["ğŸ“ Contract<br>format d'Ã©change entre services"]:1 Cd["JSON parsing Â· Redis keys"]:1
  S["ğŸš€ Context Smoke<br>chaque service dÃ©marre"]:1 Sd["@SpringBootTest Â· contextLoads"]:1
  U["ğŸ§ª Unit / Slice<br>logique mÃ©tier isolÃ©e"]:1 Ud["JUnit Â· Mockito Â· @WebMvcTest Â· Vitest"]:1
  SA["ğŸ” QualitÃ© du code â€” Hadolint, SonarCloud, terraform fmt"]:2
  SC["ğŸ›¡ï¸ SÃ©curitÃ© des dÃ©pendances â€” Trivy CVE, GitGuardian secrets"]:2

  style P fill:#e1bee7,color:#000
  style Pd fill:#f3e5f5,color:#000
  style E fill:#ce93d8,color:#000
  style Ed fill:#e1bee7,color:#000
  style D fill:#ba68c8,color:#fff
  style Dd fill:#ce93d8,color:#000
  style C fill:#ab47bc,color:#fff
  style Cd fill:#ba68c8,color:#fff
  style S fill:#9c27b0,color:#fff
  style Sd fill:#ab47bc,color:#fff
  style U fill:#7b1fa2,color:#fff
  style Ud fill:#9c27b0,color:#fff
  style SA fill:#37474f,color:#fff
  style SC fill:#263238,color:#fff
```

> Lecture de bas en haut. Les couches basses sont rapides (< 1 min) et nombreuses. Plus on monte, plus les tests sont lents et ciblÃ©s. Les deux barres sombres sont **transversales** : elles tournent en parallÃ¨le de tout le reste.

| Couche | Ce qu'elle protÃ¨ge | Exemple concret |
|---|---|---|
| Unit / Slice | La logique mÃ©tier isolÃ©e | "le parsing d'un vol OpenSky retourne les bons champs" |
| Context Smoke | Le dÃ©marrage de chaque service | "Spring Boot dÃ©marre sans erreur de configuration" |
| Contract | Le format d'Ã©change entre services | "le JSON Ã©crit par l'ingester est lisible par le processor" |
| Integration | Le fonctionnement avec une vraie base Redis | "l'ingester Ã©crit dans Redis, le dashboard relit correctement" |
| E2E Smoke | L'application dÃ©ployÃ©e en conditions rÃ©elles | "aprÃ¨s deploy, /api/flights retourne du JSON valide" |
| Performance | La tenue en charge | "10 utilisateurs simultanÃ©s, temps de rÃ©ponse < 1.5s" |
| QualitÃ© du code | Le respect des standards et bonnes pratiques | "les Dockerfiles suivent les best-practices, l'IaC est sÃ©curisÃ©e" |
| SÃ©curitÃ© dÃ©pendances | L'absence de failles connues dans les librairies | "aucune CVE critique dans les dÃ©pendances Maven/npm" |

### Chiffres clÃ©s

| Indicateur | Valeur |
|---|---|
| Tests automatisÃ©s | **52 tests** (15 fichiers, 4 langages) |
| CatÃ©gories de tests couvertes | **9** (unit, slice, integration, contract, smoke, security, quality, infra, perf) |
| Workflows GitHub Actions | **9** (dont 5 liÃ©s aux tests/qualitÃ©) |
| Services avec tests | **4/6** (ingester, processor, dashboard, frontend) |
| Ratio pyramide (unit / integ / E2E) | ~70% / 20% / 10% |

---

## 2. Shift-Left Testing

### Le concept

Dans une approche classique, les tests de sÃ©curitÃ©, de qualitÃ© et d'infrastructure sont exÃ©cutÃ©s **tard** dans le cycle : en staging, voire en production. On dÃ©couvre les problÃ¨mes aprÃ¨s avoir dÃ©ployÃ©, quand corriger coÃ»te cher en temps et en Ã©nergie.

Le **Shift-Left** inverse cette logique : on **dÃ©cale les vÃ©rifications vers la gauche** de la timeline (= vers le dÃ©but), pour attraper les erreurs le plus tÃ´t possible â€” idÃ©alement dÃ¨s que le dÃ©veloppeur pousse son code.

```mermaid
flowchart LR
  subgraph TRAD["âŒ Approche traditionnelle"]
    direction LR
    T1["Dev"] --> T2["Build"] --> T3["Deploy staging"] --> T4["Tests sÃ©cu + qualitÃ©"] --> T5["ğŸ˜± Bug trouvÃ© tard"]
  end

  subgraph SHIFT["âœ… Shift-Left (CloudRadar)"]
    direction LR
    S1["Dev"] --> S2["PR : tests + sÃ©cu + qualitÃ©"] --> S3["Build"] --> S4["Deploy"] --> S5["âœ… Confiance"]
  end

  style TRAD fill:#ffebee,stroke:#e53935,color:#000
  style SHIFT fill:#e8f5e9,stroke:#43a047,color:#000
  style T5 fill:#e53935,color:#fff
  style S2 fill:#43a047,color:#fff
  style S5 fill:#43a047,color:#fff
```

### Ce que CloudRadar applique

Dans ce projet, **8 vÃ©rifications automatiques sur 10 tournent avant le merge**. Le dÃ©veloppeur obtient un retour en quelques minutes, pas aprÃ¨s un dÃ©ploiement ratÃ©.

ConcrÃ¨tement, quand un dÃ©veloppeur ouvre une Pull Request sur CloudRadar, **4 workflows GitHub Actions se lancent en parallÃ¨le** : les tests applicatifs (Java + React), la validation Kubernetes, la vÃ©rification Terraform, et l'analyse SonarCloud. Le tout prend environ 5 minutes. Si un seul Ã©choue, le merge est bloquÃ© â€” impossible de casser `main` par accident.

```mermaid
flowchart LR
  DEV["ğŸ§‘â€ğŸ’» Dev local<br>tests Â· lint Â· format"] -->|"< 5 min"| PR["ğŸ”€ Pull Request<br>8 gates bloquantes"]
  PR -->|"tout vert"| MERGE["âœ… Build & Push<br>6 images Docker"]
  MERGE --> DEPLOY["ğŸš€ Deploy<br>infra + app + smoke"]
  DEPLOY -.-> NIGHT["ğŸŒ™ Nightly<br>test de charge"]

  style DEV fill:#c8e6c9,color:#000
  style PR fill:#bbdefb,color:#000
  style MERGE fill:#fff3e0,color:#000
  style DEPLOY fill:#fce4ec,color:#000
  style NIGHT fill:#f3e5f5,color:#000
```

> **80% des checks** se jouent sur les deux premiÃ¨res Ã©tapes (Dev + PR). Les 20% restants (build d'images, deploy, perf) ne tournent qu'aprÃ¨s le merge ou en nightly.

**Pourquoi c'est du Shift-Left ?** Traditionnellement, les tests de sÃ©curitÃ©, de qualitÃ© et d'infrastructure se font tard (en staging ou en prod). Ici, ils sont tous exÃ©cutÃ©s **sur chaque Pull Request**, avant le merge :

| Gate PR (bloquant) | Ce qu'on vÃ©rifie | Temps |
|---|---|---|
| Tests Java (3 services) | Le code mÃ©tier fonctionne | 1â€“4 min |
| Tests Frontend (Vitest) | L'interface s'affiche correctement | 20â€“60s |
| Hadolint (6 Dockerfiles) | Les images Docker suivent les bonnes pratiques | 20â€“60s |
| Trivy (dÃ©pendances) | Aucune faille de sÃ©curitÃ© connue (CVE) | 30â€“120s |
| kubeconform (manifests k8s) | Les fichiers Kubernetes sont valides | 3â€“5s |
| tfsec (Terraform) | L'infrastructure as code est sÃ©curisÃ©e | 10â€“30s |
| Terraform plan | L'infra peut Ãªtre appliquÃ©e sans erreur | 1â€“3 min |
| SonarCloud | La qualitÃ© globale du code est maintenue | 2â€“4 min |

---

## 3. Les 9 catÃ©gories de tests

Pour couvrir Ã  la fois le code applicatif, l'infrastructure cloud (Terraform, Kubernetes) et la sÃ©curitÃ© des dÃ©pendances, CloudRadar utilise **9 catÃ©gories de tests** rÃ©parties en 3 familles. Cette organisation garantit que chaque type de changement â€” qu'il touche du Java, un manifest k8s ou un module Terraform â€” est validÃ© par des checks adaptÃ©s.

```mermaid
block-beta
  columns 3

  APP["ğŸŸ¢ Code applicatif"]:3
  U["ğŸ§ª Unit / Slice<br>logique mÃ©tier isolÃ©e"]:1
  I["ğŸ”— Integration<br>services + Redis rÃ©el"]:1
  C["ğŸ“ Contract<br>format JSON entre services"]:1

  INFRA["ğŸ”µ Infrastructure & dÃ©ploiement"]:3
  IV["âš™ï¸ Infra Validation<br>Terraform + manifests k8s"]:1
  E["ğŸŒ E2E / Smoke<br>santÃ© post-dÃ©ploiement"]:1
  P["ğŸ”ï¸ Performance<br>tenue en charge (k6)"]:1

  CROSS["ğŸŸ  Transversal (toutes les PR)"]:3
  S["ğŸ”’ SÃ©curitÃ©<br>CVE Â· secrets Â· IaC"]:1
  Q["ğŸ“ QualitÃ©<br>lint Â· coverage Â· smells"]:1
  UI["ğŸ–¥ï¸ UI<br>rendu composants React"]:1

  style APP fill:#43a047,color:#fff
  style U fill:#e8f5e9,color:#000
  style I fill:#c8e6c9,color:#000
  style C fill:#a5d6a7,color:#000

  style INFRA fill:#1976d2,color:#fff
  style IV fill:#e3f2fd,color:#000
  style E fill:#bbdefb,color:#000
  style P fill:#90caf9,color:#000

  style CROSS fill:#ef6c00,color:#fff
  style S fill:#fff3e0,color:#000
  style Q fill:#ffe0b2,color:#000
  style UI fill:#ffcc80,color:#000
```

> **3 familles** : les tests **verts** valident le code applicatif (Java/React), les tests **bleus** valident l'infrastructure et le dÃ©ploiement, les tests **oranges** sont transversaux et s'exÃ©cutent sur chaque PR quelle que soit la nature du changement.

### Matrice catÃ©gorie Ã— couverture

| CatÃ©gorie | Quoi | Quand | OÃ¹ | Statut |
|---|---|---|---|---|
| ğŸ§ª **Unit / Slice** | Business logic, contrÃ´leurs, parsing | PR | `build-and-push` | âœ… ImplÃ©mentÃ© |
| ğŸ”— **Integration** | Context Spring Boot + data-path Redis | PR | `build-and-push` | âœ… ImplÃ©mentÃ© |
| ğŸ“ **Contract** | JSON serialization, Redis key format | PR | `build-and-push` | âœ… ImplÃ©mentÃ© |
| ğŸŒ **E2E / Smoke** | Health + data pipeline post-deploy | Dispatch | `ci-infra` | âœ… ImplÃ©mentÃ© |
| ğŸ”’ **Security** | DÃ©pendances CVE, secrets, IaC | PR | `build-and-push` + `ci-infra` | âœ… ImplÃ©mentÃ© |
| ğŸ“ **Code Quality** | Smells, duplication, coverage trends | PR | `sonarcloud` + `build-and-push` | âœ… ImplÃ©mentÃ© |
| âš™ï¸ **Infra Validation** | Terraform + k8s manifest schemas | PR | `ci-infra` + `ci-k8s` | âœ… ImplÃ©mentÃ© |
| ğŸ”ï¸ **Performance** | Latence p95, taux d'erreur | Nightly / dispatch | `k6-nightly-baseline` | âœ… ImplÃ©mentÃ© |
| ğŸ–¥ï¸ **UI** | Render smoke composants React | PR | `build-and-push` | âœ… ImplÃ©mentÃ© |

---

## 4. Pipelines CI/CD

Les 9 workflows GitHub Actions de CloudRadar sont organisÃ©s pour **tourner en parallÃ¨le** et donner un feedback rapide. Chaque workflow a un pÃ©rimÃ¨tre clair et un dÃ©clencheur prÃ©cis. L'authentification AWS se fait par **OIDC** (pas de clÃ©s stockÃ©es), et les builds Docker utilisent une **matrice** pour construire les 6 images en parallÃ¨le.

Voici **qui vÃ©rifie quoi, et quand** :

```mermaid
block-beta
  columns 2

  PR["ğŸ”€ Sur chaque Pull Request"]:2
  BAP["ğŸ—ï¸ build-and-push<br>Compile, teste et scanne les 6 services"]:1
  SQG["ğŸ“Š sonarcloud<br>Dette technique, couverture, duplication"]:1
  CIK["â˜¸ï¸ ci-k8s<br>Manifests Kubernetes valides ?"]:1
  CII["ğŸ”’ ci-infra<br>Terraform sÃ»r et dÃ©ployable ?"]:1

  POST["AprÃ¨s merge"]:2
  CIID["ğŸŒ ci-infra dispatch<br>Deploy complet + smoke tests"]:1
  K6["âš¡ k6 nightly<br>10 utilisateurs â€” l'app tient la charge ?"]:1

  style PR fill:#1976d2,color:#fff
  style BAP fill:#bbdefb,color:#000
  style SQG fill:#bbdefb,color:#000
  style CIK fill:#bbdefb,color:#000
  style CII fill:#bbdefb,color:#000
  style POST fill:#757575,color:#fff
  style CIID fill:#ffe0b2,color:#000
  style K6 fill:#e1bee7,color:#000
```

| Workflow | RÃ´le en une phrase | VÃ©rifie | Temps |
|---|---|---|---|
| **build-and-push** | Compiler et tester les 6 services | Tests Java Ã—3, React, lint Dockerfiles Ã—6, scan CVE | 2â€“5 min |
| **sonarcloud** | Surveiller la dette technique | Quality gate, coverage, code smells, duplication | 2â€“4 min |
| **ci-k8s** | Valider les fichiers Kubernetes | Schemas kubeconform, sync versions, noms d'images | < 1 min |
| **ci-infra** (PR) | VÃ©rifier l'infra avant dÃ©ploiement | terraform fmt/validate/plan, tfsec sÃ©curitÃ© | 1â€“3 min |
| **ci-infra** (dispatch) | DÃ©ployer et vÃ©rifier en conditions rÃ©elles | Terraform apply â†’ ArgoCD sync â†’ smoke tests | 5â€“15 min |
| **k6-nightly** | Mesurer la performance chaque nuit | p95 < 1.5s, taux d'erreur < 5%, checks > 95% | ~1 min |

> DÃ©tails de chaque workflow : voir `docs/runbooks/ci-cd/`.

---

## 5. Couverture par service

CloudRadar a 6 microservices. Les 3 services Java (ingester, processor, dashboard) concentrent la majoritÃ© des tests car ils portent la logique mÃ©tier â€” ingestion OpenSky, agrÃ©gation Redis, API REST. Le frontend React a des tests de rendu (Vitest). Les 2 services Python (health, admin-scale) sont des utilitaires lÃ©gers sans tests pour l'instant.

```mermaid
block-beta
  columns 6
  header["Couverture tests par service"]:6
  space:6
  A["dashboard"] B["ingester"] C["processor"] D["frontend"] E["health"] F["admin-scale"]
  A1["37 tests"] B1["3 tests"] C1["7 tests"] D1["5 tests"] E1["0 test"] F1["0 test"]

  style A1 fill:#4caf50,color:#fff
  style B1 fill:#4caf50,color:#fff
  style C1 fill:#4caf50,color:#fff
  style D1 fill:#4caf50,color:#fff
  style E1 fill:#f44336,color:#fff
  style F1 fill:#f44336,color:#fff
```

4 services sur 6 ont des tests automatisÃ©s (52 tests, 15 fichiers). Les 3 services Java couvrent les 3 niveaux de la pyramide : unitaire (Mockito, @WebMvcTest), intÃ©gration (Redis Testcontainers), et context smoke (@SpringBootTest). Le frontend couvre le rendu composant (Vitest + Testing Library).

Les contrats inter-services (clÃ©s Redis, format JSON) sont validÃ©s par des tests Testcontainers dÃ©diÃ©s dans chaque service â€” documentÃ©s dans `docs/events-schemas/redis-keys.md`.

SonarCloud ingÃ¨re la couverture Java (JaCoCo) et frontend (lcov) pour un suivi de tendance unifiÃ©.

---

## 6. Workflow Ã— catÃ©gorie : qui vÃ©rifie quoi ?

Cette matrice croise les 6 workflows avec les 9 catÃ©gories de tests. Elle permet de vÃ©rifier d'un coup d'Å“il qu'**aucune catÃ©gorie n'est orpheline** â€” chaque type de vÃ©rification est portÃ© par au moins un workflow.

| CatÃ©gorie | build-and-push | sonarcloud | ci-k8s | ci-infra PR | ci-infra deploy | k6 nightly |
|---|:---:|:---:|:---:|:---:|:---:|:---:|
| ğŸ§ª Unit | ğŸŸ¢ | | | | | |
| ğŸ”— Integ | ğŸŸ¢ | | | | | |
| ğŸ“ Contract | ğŸŸ¢ | | | | | |
| ğŸ–¥ï¸ UI | ğŸŸ¢ | | | | | |
| ğŸ”’ SÃ©cu | ğŸŸ¢ | | | ğŸŸ¢ | | |
| ğŸ“ QualitÃ© | | ğŸŸ¢ | | | | |
| âš™ï¸ Infra | | | ğŸŸ¢ | ğŸŸ¢ | | |
| ğŸŒ E2E | | | | | ğŸŸ¢ | |
| ğŸ”ï¸ Perf | | | | | | ğŸŸ¢ |

> `build-and-push` porte **5/9 catÃ©gories**. Toutes les catÃ©gories sont couvertes par au moins un workflow.

---

## 7. AmÃ©liorations possibles

```mermaid
quadrantChart
  title Rapport effort / impact des amÃ©liorations
  x-axis "Effort faible" --> "Effort Ã©levÃ©"
  y-axis "Impact faible" --> "Impact Ã©levÃ©"

  "Dependabot": [0.08, 0.80]
  "SpotBugs": [0.15, 0.70]
  "ESLint + Prettier": [0.15, 0.60]
  "Trivy image": [0.08, 0.65]
  "Tests Python": [0.35, 0.55]
  "HTTP contracts": [0.50, 0.55]
  "Coverage gate": [0.05, 0.50]
  "Rollback validation": [0.25, 0.45]
  "Frontend E2E": [0.65, 0.40]
  "Mutation testing": [0.40, 0.30]
  "Chaos testing": [0.85, 0.25]
```

| PrioritÃ© | AmÃ©lioration | Pourquoi | Effort |
|---|---|---|---|
| ğŸ”´ Haute | **Dependabot** | PR automatiques de mise Ã  jour dÃ©pendances (Maven, npm, Actions) | ~15 min |
| ğŸ”´ Haute | **SpotBugs** | DÃ©tection statique NPE, concurrence, anti-patterns Java | ~30 min |
| ğŸ”´ Haute | **ESLint + Prettier** | Aucun linting TypeScript/React en CI | ~30 min |
| ğŸ”´ Haute | **Trivy image** | Scan couches OS/runtime des images Docker (seul Trivy fs existe) | ~15 min |
| ğŸŸ¡ Moyenne | **Tests Python** | 2/6 services sans tests (health, admin-scale) | ~2h |
| ğŸŸ¡ Moyenne | **HTTP contract tests** | Parsing OpenSky et payload `/api/flights` testÃ©s sans serveur HTTP mock | ~3h |
| ğŸŸ¡ Moyenne | **Coverage enforcement** | Activer le seuil bloquant SonarCloud sur le nouveau code | ~10 min |
| ğŸ”µ Basse | **Rollback validation** | Aucune vÃ©rification de la capacitÃ© de rollback ArgoCD | ~1h |
| ğŸ”µ Basse | **Frontend E2E** | Pas de test navigateur rÃ©el (Playwright/Cypress) | ~4h |
| ğŸ”µ Basse | **Mutation testing** | VÃ©rifier que les tests dÃ©tectent rÃ©ellement les rÃ©gressions (PIT) | ~2h |
