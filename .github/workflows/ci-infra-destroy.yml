name: ci-infra-destroy

on:
  workflow_dispatch:
    inputs:
      environment:
        description: "Target environment to destroy (dev or prod)"
        required: true
        type: choice
        options:
          - dev
          - prod
      redis_backup_restore:
        description: "Run Redis backup + rotation before destroy (dev only)"
        required: true
        type: boolean
        default: true
      confirm_destroy:
        description: "Type DESTROY to confirm"
        required: true
        default: ""

permissions:
  id-token: write
  contents: read

env:
  # Repo variables provide the shared backend and role configuration.
  AWS_REGION: ${{ vars.AWS_REGION || 'us-east-1' }}
  TF_STATE_BUCKET: ${{ vars.TF_STATE_BUCKET }}
  TF_LOCK_TABLE_NAME: ${{ vars.TF_LOCK_TABLE_NAME }}
  TF_BACKUP_BUCKET_NAME: ${{ vars.TF_BACKUP_BUCKET_NAME }}
  TF_ROLE_ARN: ${{ vars.AWS_TERRAFORM_ROLE_ARN }}

jobs:
  destroy:
    runs-on: ubuntu-latest
    environment: ${{ inputs.environment }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Select environment
        run: |
          # Set the Terraform root and state key for the selected env.
          case "${{ inputs.environment }}" in
            dev)
              echo "TF_DIR=infra/aws/live/dev" >> "$GITHUB_ENV"
              echo "TF_KEY=cloudradar/dev/terraform.tfstate" >> "$GITHUB_ENV"
              ;;
            prod)
              echo "TF_DIR=infra/aws/live/prod" >> "$GITHUB_ENV"
              echo "TF_KEY=cloudradar/prod/terraform.tfstate" >> "$GITHUB_ENV"
              ;;
          esac

      - name: Guard destroy confirmation
        if: ${{ inputs.confirm_destroy != 'DESTROY' }}
        run: |
          # Hard guard to avoid accidental destroys.
          echo "Set confirm_destroy=DESTROY to proceed with destroy."
          exit 1

      - name: Terraform init (remote backend)
        run: |
          # Use the shared S3/DynamoDB backend for state + locking.
          terraform -chdir="${TF_DIR}" init \
            -backend-config="bucket=${TF_STATE_BUCKET}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=${TF_LOCK_TABLE_NAME}" \
            -backend-config="key=${TF_KEY}"

      - name: Terraform validate
        run: terraform -chdir="${TF_DIR}" validate

      - name: Disable CloudWatch alarm actions (best effort)
        run: |
          set -euo pipefail
          ALARM_PREFIX="cloudradar-${{ inputs.environment }}-"
          mapfile -t alarms < <(aws cloudwatch describe-alarms \
            --query "MetricAlarms[?starts_with(AlarmName, \`${ALARM_PREFIX}\`) && contains(AlarmName, \`status-check-failed\`)].AlarmName" \
            --output text | tr '\t' '\n' | sed '/^$/d')

          if (( ${#alarms[@]} == 0 )); then
            echo "::notice title=cloudwatch-alerting::No status-check alarms found for prefix ${ALARM_PREFIX}."
            exit 0
          fi

          aws cloudwatch disable-alarm-actions --alarm-names "${alarms[@]}"
          echo "::notice title=cloudwatch-alerting::Disabled alarm actions for ${#alarms[@]} alarm(s)."

      - name: Mute in-cluster Alertmanager (dev best effort)
        if: ${{ inputs.environment == 'dev' }}
        run: |
          set -euo pipefail
          K3S_SERVER_INSTANCE_ID="$(terraform -chdir="${TF_DIR}" output -raw k3s_server_instance_id)"
          if [[ -z "${K3S_SERVER_INSTANCE_ID}" ]]; then
            echo "::notice title=alertmanager::k3s_server_instance_id missing; skipping Alertmanager mute."
            exit 0
          fi

          params="$(jq -n \
            --arg c0 'i=0; while [ $i -lt 30 ]; do if [ -x /usr/local/bin/kubectl ]; then break; fi; echo \"Waiting for kubectl...\"; sleep 10; i=$((i+1)); done; if [ ! -x /usr/local/bin/kubectl ]; then echo \"kubectl not found after 300s\"; exit 1; fi' \
            --arg c1 'export KUBECONFIG=/etc/rancher/k3s/k3s.yaml' \
            --arg c2 'sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl -n monitoring scale statefulset/alertmanager-kube-prometheus-alertmanager --replicas=0 || true' \
            '{commands:[$c0,$c1,$c2]}')"

          mute_cmd_id="$(aws ssm send-command \
            --instance-ids "${K3S_SERVER_INSTANCE_ID}" \
            --document-name "AWS-RunShellScript" \
            --parameters "${params}" \
            --timeout-seconds 300 \
            --query "Command.CommandId" \
            --output text)"

          echo "::notice title=SSM::Mute Alertmanager command_id=${mute_cmd_id}"
          if ! aws ssm wait command-executed --command-id "${mute_cmd_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}"; then
            aws ssm get-command-invocation --command-id "${mute_cmd_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}" || true
            echo "::warning title=alertmanager::Failed to scale Alertmanager down; continuing destroy."
          fi

      - name: Backup Redis + rotate (dev only)
        if: ${{ inputs.environment == 'dev' }}
        shell: bash
        env:
          BACKUP_BUCKET: ${{ env.TF_BACKUP_BUCKET_NAME }}
        run: |
          set -euo pipefail

          if [[ "${{ inputs.redis_backup_restore }}" != "true" ]]; then
            echo "::notice title=Redis backup::redis_backup_restore=${{ inputs.redis_backup_restore }}; skipping Redis backup/rotation."
            exit 0
          fi

          if [[ -z "${BACKUP_BUCKET}" ]]; then
            echo "::notice title=Redis backup::TF_BACKUP_BUCKET_NAME not set; skipping Redis backup/rotation."
            exit 0
          fi

          echo "Loading k3s_server_instance_id from Terraform outputs..."
          K3S_SERVER_INSTANCE_ID="$(terraform -chdir="${TF_DIR}" output -raw k3s_server_instance_id)"
          if [[ -z "${K3S_SERVER_INSTANCE_ID}" ]]; then
            echo "ERROR: k3s_server_instance_id is empty" >&2
            exit 1
          fi

          echo "Waiting for SSM instance readiness..."
          instance_id="${K3S_SERVER_INSTANCE_ID}"
          max_attempts=12
          attempt=1
          sleep_seconds=10

          while [ "${attempt}" -le "${max_attempts}" ]; do
            state="$(aws ec2 describe-instance-status \
              --instance-ids "${instance_id}" \
              --include-all-instances \
              --query "InstanceStatuses[0].InstanceState.Name" \
              --output text 2>/dev/null || true)"

            ping="$(aws ssm describe-instance-information \
              --filters "Key=InstanceIds,Values=${instance_id}" \
              --query "InstanceInformationList[0].PingStatus" \
              --output text 2>/dev/null || true)"

            if [ "${state}" = "running" ] && [ "${ping}" = "Online" ]; then
              echo "SSM instance is online."
              break
            fi

            echo "Waiting for SSM instance readiness (state=${state:-unknown} ping=${ping:-unknown})..."
            sleep "${sleep_seconds}"
            attempt=$((attempt + 1))
            sleep_seconds=$((sleep_seconds + 5))
          done

          if [ "${attempt}" -gt "${max_attempts}" ]; then
            echo "SSM instance not ready after ${max_attempts} attempts."
            exit 1
          fi

          TS="$(date -u +%Y%m%dT%H%M%SZ)"
          S3_PREFIX="redis-backups/${TS}"
          echo "Backup timestamp: ${TS}"
          echo "Backup destination: s3://${BACKUP_BUCKET}/${S3_PREFIX}/"

          # Stop writes (best effort).
          params="$(jq -n \
            --arg c0 'i=0; while [ $i -lt 30 ]; do if [ -x /usr/local/bin/kubectl ]; then break; fi; echo "Waiting for kubectl..."; sleep 10; i=$((i+1)); done; if [ ! -x /usr/local/bin/kubectl ]; then echo "kubectl not found after 300s"; exit 1; fi' \
            --arg c1 "export KUBECONFIG=/etc/rancher/k3s/k3s.yaml" \
            --arg c2 "sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl -n cloudradar scale deploy/ingester --replicas=0 || true" \
            '{commands:[$c0,$c1,$c2]}')"

          stop_cmd_id="$(aws ssm send-command \
            --instance-ids "${K3S_SERVER_INSTANCE_ID}" \
            --document-name "AWS-RunShellScript" \
            --parameters "${params}" \
            --timeout-seconds 300 \
            --query "Command.CommandId" \
            --output text)"

          echo "::notice title=SSM::Stop ingester command_id=${stop_cmd_id}"
          if ! aws ssm wait command-executed --command-id "${stop_cmd_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}"; then
            aws ssm get-command-invocation --command-id "${stop_cmd_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}" || true
            echo "WARN: Failed to scale ingester to 0; continuing with backup."
          fi

          # Backup.
          BACKUP_B64="$(base64 -w0 scripts/redis-backup.sh)"
          params="$(jq -n \
            --arg c0 'i=0; while [ $i -lt 30 ]; do if [ -x /usr/local/bin/kubectl ]; then break; fi; echo "Waiting for kubectl..."; sleep 10; i=$((i+1)); done; if [ ! -x /usr/local/bin/kubectl ]; then echo "kubectl not found after 300s"; exit 1; fi' \
            --arg c1 "export KUBECONFIG=/etc/rancher/k3s/k3s.yaml" \
            --arg c2 "echo '${BACKUP_B64}' | base64 -d > /tmp/redis-backup.sh && chmod +x /tmp/redis-backup.sh" \
            --arg c3 "/tmp/redis-backup.sh --kubeconfig /etc/rancher/k3s/k3s.yaml --kubectl /usr/local/bin/kubectl --s3-bucket ${BACKUP_BUCKET} --s3-prefix ${S3_PREFIX} --region ${AWS_REGION}" \
            '{commands:[$c0,$c1,$c2,$c3]}')"

          backup_cmd_id="$(aws ssm send-command \
            --instance-ids "${K3S_SERVER_INSTANCE_ID}" \
            --document-name "AWS-RunShellScript" \
            --parameters "${params}" \
            --timeout-seconds 900 \
            --query "Command.CommandId" \
            --output text)"

          echo "::notice title=SSM::Redis backup command_id=${backup_cmd_id}"
          if ! aws ssm wait command-executed --command-id "${backup_cmd_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}"; then
            aws ssm get-command-invocation --command-id "${backup_cmd_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}" || true
            exit 1
          fi

          # Rotate backups (keep last 3).
          echo "Rotating Redis backups in s3://${BACKUP_BUCKET}/redis-backups/ (keep last 3)..."
          mapfile -t prefixes < <(aws s3api list-objects-v2 \
            --bucket "${BACKUP_BUCKET}" \
            --prefix "redis-backups/" \
            --delimiter "/" \
            --query "CommonPrefixes[].Prefix" \
            --output text | tr '\t' '\n' | sed '/^$/d' || true)

          if (( ${#prefixes[@]} <= 3 )); then
            echo "Found ${#prefixes[@]} backups; nothing to rotate."
            exit 0
          fi

          mapfile -t sorted < <(printf "%s\n" "${prefixes[@]}" | sort)
          delete_count=$(( ${#sorted[@]} - 3 ))
          for ((i=0; i<delete_count; i++)); do
            echo "Deleting old backup prefix: s3://${BACKUP_BUCKET}/${sorted[$i]}"
            aws s3 rm "s3://${BACKUP_BUCKET}/${sorted[$i]}" --recursive
          done

      - name: Terraform destroy
        run: terraform -chdir="${TF_DIR}" destroy -input=false -auto-approve -var-file=terraform.tfvars

      - name: Orphan scan post-destroy (best effort)
        if: ${{ always() }}
        continue-on-error: true
        run: |
          scripts/ci/find-orphans.sh \
            --environment "${{ inputs.environment }}" \
            --tf-dir "${TF_DIR}" \
            --mode post-destroy \
            --strict false \
            --summary-path "$GITHUB_STEP_SUMMARY"
