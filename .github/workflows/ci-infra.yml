name: ci-infra

# Workflow overview:
# - PR: fmt/validate/plan/module validation/security scan.
# - Dispatch: select env -> validate -> plan -> apply -> load outputs -> k3s check -> ArgoCD bootstrap -> smoke tests.

on:
  pull_request:
    paths:
      - "infra/**"
      - ".github/workflows/ci-infra.yml"
  workflow_dispatch:
    inputs:
      environment:
        description: "Target environment to apply (dev or prod)"
        required: true
        type: choice
        options:
          - dev
          - prod
      auto_approve:
        description: "Set to true to run apply"
        required: true
        default: "false"
      run_smoke_tests:
        description: "Set to true to run post-apply network smoke tests"
        required: true
        default: "false"
      backup_bucket_name:
        description: "Optional S3 bucket name for SQLite backups (dev only)"
        required: false

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: ${{ vars.AWS_REGION || 'us-east-1' }}
  TF_STATE_BUCKET: ${{ vars.TF_STATE_BUCKET }}
  TF_LOCK_TABLE_NAME: ${{ vars.TF_LOCK_TABLE_NAME }}
  TF_BACKUP_BUCKET_NAME: ${{ vars.TF_BACKUP_BUCKET_NAME }}
  TF_ROLE_ARN: ${{ vars.AWS_TERRAFORM_ROLE_ARN }}

jobs:
  fmt:
    if: ${{ github.event_name == 'pull_request' }}
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Terraform fmt
        run: terraform fmt -check -recursive infra/aws

  validate-plan:
    if: ${{ github.event_name == 'pull_request' }}
    runs-on: ubuntu-latest
    needs: fmt
    strategy:
      matrix:
        include:
          - name: bootstrap
            dir: infra/aws/bootstrap
            backend: false
          - name: live-dev
            dir: infra/aws/live/dev
            backend: true
            key: cloudradar/dev/terraform.tfstate
          - name: live-prod
            dir: infra/aws/live/prod
            backend: true
            key: cloudradar/prod/terraform.tfstate
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Terraform init (local backend)
        if: ${{ matrix.backend == false }}
        run: terraform -chdir=${{ matrix.dir }} init -backend=false

      - name: Terraform init (remote backend)
        if: ${{ matrix.backend == true }}
        run: |
          terraform -chdir=${{ matrix.dir }} init \
            -backend-config="bucket=${TF_STATE_BUCKET}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=${TF_LOCK_TABLE_NAME}" \
            -backend-config="key=${{ matrix.key }}"

      - name: Terraform validate
        run: terraform -chdir=${{ matrix.dir }} validate

      - name: Terraform plan
        if: ${{ matrix.name == 'bootstrap' }}
        run: |
          set -euo pipefail
          BACKUP_BUCKET_NAME="${TF_BACKUP_BUCKET_NAME:-}"
          EXTRA_VARS=()

          if [[ -n "${BACKUP_BUCKET_NAME}" ]]; then
            EXTRA_VARS+=("-var=backup_bucket_name=${BACKUP_BUCKET_NAME}")
          fi

          terraform -chdir=${{ matrix.dir }} plan -input=false -no-color \
            -var="region=${AWS_REGION}" \
            -var="state_bucket_name=${TF_STATE_BUCKET}" \
            -var="lock_table_name=${TF_LOCK_TABLE_NAME}" \
            "${EXTRA_VARS[@]}"

      - name: Terraform plan
        if: ${{ matrix.name != 'bootstrap' }}
        run: |
          set -euo pipefail
          BACKUP_BUCKET_NAME="${TF_BACKUP_BUCKET_NAME:-}"
          EXTRA_VARS=()

          if [[ "${{ matrix.name }}" == "live-dev" && -n "${BACKUP_BUCKET_NAME}" ]]; then
            EXTRA_VARS+=("-var=sqlite_backup_bucket_name=${BACKUP_BUCKET_NAME}")
          fi

          terraform -chdir=${{ matrix.dir }} plan -input=false -no-color -var-file=terraform.tfvars.example \
            "${EXTRA_VARS[@]}"

  validate-modules:
    if: ${{ github.event_name == 'pull_request' }}
    runs-on: ubuntu-latest
    needs: fmt
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Terraform validate modules
        run: |
          shopt -s nullglob
          for module in infra/aws/modules/*; do
            if ls "${module}"/*.tf >/dev/null 2>&1; then
              terraform -chdir="${module}" init -backend=false
              terraform -chdir="${module}" validate
            fi
          done

  tfsec:
    if: ${{ github.event_name == 'pull_request' }}
    runs-on: ubuntu-latest
    needs: fmt
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: tfsec scan
        uses: aquasecurity/tfsec-action@v1.0.3
        with:
          working_directory: infra/aws
          github_token: ${{ github.token }}
          additional_args: >-
            --exclude aws-s3-encryption-customer-key,aws-dynamodb-enable-at-rest-encryption,aws-dynamodb-enable-recovery,aws-s3-enable-bucket-logging,aws-dynamodb-table-customer-key

  env-select:
    if: ${{ github.event_name == 'workflow_dispatch' }}
    runs-on: ubuntu-latest
    outputs:
      tf_dir: ${{ steps.select.outputs.tf_dir }}
      tf_key: ${{ steps.select.outputs.tf_key }}
    steps:
      - name: Select environment
        id: select
        run: |
          case "${{ inputs.environment }}" in
            dev)
              echo "tf_dir=infra/aws/live/dev" >> "$GITHUB_OUTPUT"
              echo "tf_key=cloudradar/dev/terraform.tfstate" >> "$GITHUB_OUTPUT"
              ;;
            prod)
              echo "tf_dir=infra/aws/live/prod" >> "$GITHUB_OUTPUT"
              echo "tf_key=cloudradar/prod/terraform.tfstate" >> "$GITHUB_OUTPUT"
              ;;
          esac

  tf-validate:
    if: ${{ github.event_name == 'workflow_dispatch' }}
    runs-on: ubuntu-latest
    needs: env-select
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Terraform init (remote backend)
        run: |
          terraform -chdir="${{ needs.env-select.outputs.tf_dir }}" init \
            -backend-config="bucket=${TF_STATE_BUCKET}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=${TF_LOCK_TABLE_NAME}" \
            -backend-config="key=${{ needs.env-select.outputs.tf_key }}"

      - name: Terraform validate
        run: terraform -chdir="${{ needs.env-select.outputs.tf_dir }}" validate

  tf-plan:
    if: ${{ github.event_name == 'workflow_dispatch' }}
    runs-on: ubuntu-latest
    needs:
      - env-select
      - tf-validate
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Terraform init (remote backend)
        run: |
          terraform -chdir="${{ needs.env-select.outputs.tf_dir }}" init \
            -backend-config="bucket=${TF_STATE_BUCKET}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=${TF_LOCK_TABLE_NAME}" \
            -backend-config="key=${{ needs.env-select.outputs.tf_key }}"

      - name: Terraform plan
        run: |
          set -euo pipefail
          BACKUP_BUCKET_NAME="${{ inputs.backup_bucket_name || env.TF_BACKUP_BUCKET_NAME }}"
          EXTRA_VARS=()

          if [[ "${{ inputs.environment }}" == "dev" && -n "${BACKUP_BUCKET_NAME}" ]]; then
            EXTRA_VARS+=("-var=sqlite_backup_bucket_name=${BACKUP_BUCKET_NAME}")
          fi

          terraform -chdir="${{ needs.env-select.outputs.tf_dir }}" plan -input=false -no-color -var-file=terraform.tfvars.example \
            "${EXTRA_VARS[@]}"

  tf-apply:
    if: ${{ github.event_name == 'workflow_dispatch' }}
    runs-on: ubuntu-latest
    needs:
      - env-select
      - tf-plan
    environment: ${{ inputs.environment }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Guard apply confirmation
        if: ${{ inputs.auto_approve != 'true' }}
        run: |
          echo "Set auto_approve=true to proceed with apply."
          exit 1

      - name: Terraform init (remote backend)
        run: |
          terraform -chdir="${{ needs.env-select.outputs.tf_dir }}" init \
            -backend-config="bucket=${TF_STATE_BUCKET}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=${TF_LOCK_TABLE_NAME}" \
            -backend-config="key=${{ needs.env-select.outputs.tf_key }}"

      - name: Terraform apply
        run: |
          set -euo pipefail
          BACKUP_BUCKET_NAME="${{ inputs.backup_bucket_name || env.TF_BACKUP_BUCKET_NAME }}"
          EXTRA_VARS=()

          if [[ "${{ inputs.environment }}" == "dev" && -n "${BACKUP_BUCKET_NAME}" ]]; then
            EXTRA_VARS+=("-var=sqlite_backup_bucket_name=${BACKUP_BUCKET_NAME}")
          fi

          terraform -chdir="${{ needs.env-select.outputs.tf_dir }}" apply -input=false -auto-approve -var-file=terraform.tfvars.example \
            "${EXTRA_VARS[@]}"

  tf-outputs:
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.environment == 'dev' }}
    runs-on: ubuntu-latest
    needs:
      - env-select
      - tf-apply
    outputs:
      k3s_server_instance_id: ${{ steps.outputs.outputs.k3s_server_instance_id }}
      edge_instance_id: ${{ steps.outputs.outputs.edge_instance_id }}
      edge_public_ip: ${{ steps.outputs.outputs.edge_public_ip }}
      edge_basic_auth_user: ${{ steps.outputs.outputs.edge_basic_auth_user }}
      edge_basic_auth_ssm_parameter_name: ${{ steps.outputs.outputs.edge_basic_auth_ssm_parameter_name }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Terraform init (remote backend)
        run: |
          terraform -chdir="${{ needs.env-select.outputs.tf_dir }}" init \
            -backend-config="bucket=${TF_STATE_BUCKET}" \
            -backend-config="region=${AWS_REGION}" \
            -backend-config="dynamodb_table=${TF_LOCK_TABLE_NAME}" \
            -backend-config="key=${{ needs.env-select.outputs.tf_key }}"

      - name: Load outputs for ArgoCD bootstrap
        id: outputs
        run: |
          outputs_json="$(terraform -chdir="${{ needs.env-select.outputs.tf_dir }}" output -json)"
          echo "k3s_server_instance_id=$(jq -r '.k3s_server_instance_id.value' <<< "${outputs_json}")" >> "$GITHUB_OUTPUT"
          echo "edge_instance_id=$(jq -r '.edge_instance_id.value' <<< "${outputs_json}")" >> "$GITHUB_OUTPUT"
          echo "edge_public_ip=$(jq -r '.edge_public_ip.value' <<< "${outputs_json}")" >> "$GITHUB_OUTPUT"
          echo "edge_basic_auth_user=$(jq -r '.edge_basic_auth_user.value' <<< "${outputs_json}")" >> "$GITHUB_OUTPUT"
          echo "edge_basic_auth_ssm_parameter_name=$(jq -r '.edge_basic_auth_ssm_parameter_name.value' <<< "${outputs_json}")" >> "$GITHUB_OUTPUT"

  k3s-ready-check:
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.environment == 'dev' && inputs.run_smoke_tests == 'true' }}
    runs-on: ubuntu-latest
    needs: tf-outputs
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Verify k3s readiness via SSM
        env:
          K3S_SERVER_INSTANCE_ID: ${{ needs.tf-outputs.outputs.k3s_server_instance_id }}
        run: |
          # AWS-RunShellScript executes with /bin/sh; keep SSM commands POSIX-only.
          params="$(jq -cn \
            --arg c0 "i=0; while [ \$i -lt 30 ]; do if [ -x /usr/local/bin/kubectl ]; then break; fi; echo \"Waiting for kubectl...\"; sleep 10; i=\$((i+1)); done; if [ ! -x /usr/local/bin/kubectl ]; then echo \"kubectl not found after 300s\"; exit 1; fi" \
            --arg c1 "export KUBECONFIG=/etc/rancher/k3s/k3s.yaml" \
            --arg c2 "sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl wait --for=condition=Ready node --all --timeout=300s" \
            --arg c3 "sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl get nodes -o wide" \
            '{commands: [$c0, $c1, $c2, $c3]}')"

          max_attempts=5
          attempt=1
          sleep_seconds=10

          while [ "${attempt}" -le "${max_attempts}" ]; do
            echo "k3s-ready-check attempt ${attempt}/${max_attempts}"

            command_id="$(aws ssm send-command \
              --instance-ids "${K3S_SERVER_INSTANCE_ID}" \
              --document-name "AWS-RunShellScript" \
              --parameters "${params}" \
              --query "Command.CommandId" \
              --output text)"

            echo "::notice title=SSM::k3s readiness command_id=${command_id} attempt=${attempt}"

            if aws ssm wait command-executed --command-id "${command_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}"; then
              status="$(aws ssm get-command-invocation \
                --command-id "${command_id}" \
                --instance-id "${K3S_SERVER_INSTANCE_ID}" \
                --query "Status" \
                --output text)"

              aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}"

              if [ "${status}" = "Success" ]; then
                echo "k3s-ready-check succeeded"
                exit 0
              fi

              echo "k3s-ready-check status=${status}"
            else
              echo "k3s-ready-check wait failed for command_id=${command_id}"
            fi

            if [ "${attempt}" -lt "${max_attempts}" ]; then
              echo "Retrying in ${sleep_seconds}s..."
              sleep "${sleep_seconds}"
            fi

            attempt=$((attempt + 1))
            sleep_seconds=$((sleep_seconds + 5))
          done

          echo "k3s-ready-check failed after ${max_attempts} attempts"
          exit 1

  argocd-bootstrap:
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.environment == 'dev' }}
    runs-on: ubuntu-latest
    needs:
      - tf-outputs
      - k3s-ready-check
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Bootstrap ArgoCD via SSM
        run: scripts/bootstrap-argocd.sh "${{ needs.tf-outputs.outputs.k3s_server_instance_id }}" "${AWS_REGION}"

  smoke-tests:
    if: ${{ github.event_name == 'workflow_dispatch' && inputs.environment == 'dev' && inputs.run_smoke_tests == 'true' }}
    runs-on: ubuntu-latest
    needs:
      - argocd-bootstrap
      - tf-outputs
      - k3s-ready-check
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.TF_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Wait for ArgoCD app sync
        env:
          K3S_SERVER_INSTANCE_ID: ${{ needs.tf-outputs.outputs.k3s_server_instance_id }}
        run: |
          params="$(jq -cn \
            --arg c0 "i=0; while [ \$i -lt 30 ]; do if [ -x /usr/local/bin/kubectl ]; then break; fi; echo \"Waiting for kubectl...\"; sleep 10; i=\$((i+1)); done; if [ ! -x /usr/local/bin/kubectl ]; then echo \"kubectl not found after 300s\"; exit 1; fi" \
            --arg c1 "export KUBECONFIG=/etc/rancher/k3s/k3s.yaml" \
            --arg c2 "set -eu" \
            --arg c3 "i=0; while [ \$i -lt 30 ]; do sync=\$(sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl -n argocd get application cloudradar -o jsonpath={.status.sync.status} 2>/dev/null || true); health=\$(sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl -n argocd get application cloudradar -o jsonpath={.status.health.status} 2>/dev/null || true); if [ \"\$sync\" = \"Synced\" ] && [ \"\$health\" = \"Healthy\" ]; then echo \"ArgoCD app is synced and healthy\"; exit 0; fi; echo \"Waiting for ArgoCD app (sync=\$sync health=\$health)\"; sleep 10; i=\$((i+1)); done; echo \"ArgoCD app not ready\"; exit 1" \
            '{commands: [$c0, $c1, $c2, $c3]}')"

          command_id="$(aws ssm send-command \
            --instance-ids "${K3S_SERVER_INSTANCE_ID}" \
            --document-name "AWS-RunShellScript" \
            --parameters "${params}" \
            --query "Command.CommandId" \
            --output text)"

          echo "::notice title=SSM::argocd app sync command_id=${command_id}"

          aws ssm wait command-executed --command-id "${command_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}"
          aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}"

      - name: Wait for healthz deployment
        env:
          K3S_SERVER_INSTANCE_ID: ${{ needs.tf-outputs.outputs.k3s_server_instance_id }}
          EDGE_PUBLIC_IP: ${{ needs.tf-outputs.outputs.edge_public_ip }}
          EDGE_BASIC_AUTH_USER: ${{ needs.tf-outputs.outputs.edge_basic_auth_user }}
          EDGE_BASIC_AUTH_SSM_PARAMETER_NAME: ${{ needs.tf-outputs.outputs.edge_basic_auth_ssm_parameter_name }}
        run: |
          params="$(jq -cn \
            --arg c0 "i=0; while [ \$i -lt 30 ]; do if [ -x /usr/local/bin/kubectl ]; then break; fi; echo \"Waiting for kubectl...\"; sleep 10; i=\$((i+1)); done; if [ ! -x /usr/local/bin/kubectl ]; then echo \"kubectl not found after 300s\"; exit 1; fi" \
            --arg c1 "export KUBECONFIG=/etc/rancher/k3s/k3s.yaml" \
            --arg c2 "sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl -n cloudradar rollout status deploy/healthz --timeout=300s" \
            --arg c3 "sudo --preserve-env=KUBECONFIG /usr/local/bin/kubectl -n cloudradar get deploy/healthz" \
            '{commands: [$c0, $c1, $c2, $c3]}')"

          command_id="$(aws ssm send-command \
            --instance-ids "${K3S_SERVER_INSTANCE_ID}" \
            --document-name "AWS-RunShellScript" \
            --parameters "${params}" \
            --query "Command.CommandId" \
            --output text)"

          echo "::notice title=SSM::healthz rollout command_id=${command_id}"

          aws ssm wait command-executed --command-id "${command_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}"
          aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${K3S_SERVER_INSTANCE_ID}"

      - name: Verify /healthz from the Internet
        env:
          EDGE_INSTANCE_ID: ${{ needs.tf-outputs.outputs.edge_instance_id }}
          EDGE_PUBLIC_IP: ${{ needs.tf-outputs.outputs.edge_public_ip }}
          EDGE_BASIC_AUTH_USER: ${{ needs.tf-outputs.outputs.edge_basic_auth_user }}
          EDGE_BASIC_AUTH_SSM_PARAMETER_NAME: ${{ needs.tf-outputs.outputs.edge_basic_auth_ssm_parameter_name }}
        run: |
          params="$(jq -cn \
            --arg c0 "set -e" \
            --arg c1 "systemctl is-active nginx" \
            --arg c2 "ss -ltnp | grep -E ':443'" \
            '{commands: [$c0, $c1, $c2]}')"

          i=0
          while [ $i -lt 3 ]; do
            command_id="$(aws ssm send-command \
              --instance-ids "${EDGE_INSTANCE_ID}" \
              --document-name "AWS-RunShellScript" \
              --parameters "${params}" \
              --query "Command.CommandId" \
              --output text)"

            echo "::notice title=SSM::edge nginx check attempt=$((i+1)) command_id=${command_id}"

            if aws ssm wait command-executed --command-id "${command_id}" --instance-id "${EDGE_INSTANCE_ID}"; then
              aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${EDGE_INSTANCE_ID}"
              break
            fi

            aws ssm get-command-invocation --command-id "${command_id}" --instance-id "${EDGE_INSTANCE_ID}" || true
            i=$((i+1))
            if [ $i -lt 3 ]; then
              echo "Edge nginx not ready, retrying in 10s..."
              sleep 10
            else
              echo "Edge nginx not ready after 3 attempts."
              exit 1
            fi
          done

          edge_basic_auth_password="$(aws ssm get-parameter \
            --name "${EDGE_BASIC_AUTH_SSM_PARAMETER_NAME}" \
            --with-decryption \
            --query "Parameter.Value" \
            --output text)"

          echo "::add-mask::${edge_basic_auth_password}"

          status_code="$(curl -k -s -o /dev/null -w '%{http_code}' \
            --connect-timeout 5 \
            --max-time 10 \
            --retry 2 \
            --retry-delay 2 \
            --retry-all-errors \
            -u "${EDGE_BASIC_AUTH_USER}:${edge_basic_auth_password}" \
            "https://${EDGE_PUBLIC_IP}/healthz")"

          if [[ "${status_code}" != "200" ]]; then
            echo "Expected 200 from /healthz, got ${status_code}."
            exit 1
          fi
